<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Welcome to Levine&#39;s Wikis</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Welcome to Levine's Wikis">
<meta property="og:url" content="http://levinehuang.github.io/index.html">
<meta property="og:site_name" content="Welcome to Levine's Wikis">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Welcome to Levine's Wikis">
  
    <link rel="alternate" href="/atom.xml" title="Welcome to Levine&#39;s Wikis" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/wiki/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/wiki/" id="logo">Welcome to Levine&#39;s Wikis</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/wiki/">Home</a>
        
          <a class="main-nav-link" href="/wiki/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://levinehuang.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-A01-BigData/02-Spark/SparkSQL" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/03/22/A01-BigData/02-Spark/SparkSQL/" class="article-date">
  <time datetime="2017-03-21T23:54:47.106Z" itemprop="datePublished">2017-03-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>sql sqlText sqlParser   UnresolvedLogicalPlan<br>    Analyzer ResolvedLogicalPlan<br>    Optimizer OptimizedLogicalPlan<br>    SparkPlanner PhysicalPlan<br>              prepareForExcution<br>              execute</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/03/22/A01-BigData/02-Spark/SparkSQL/" data-id="cj2egklrd001hsco0jnmbd2h2" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-A05-MI/10-DL理论/DL之05-RNN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/03/09/A05-MI/10-DL理论/DL之05-RNN/" class="article-date">
  <time datetime="2017-03-09T00:58:06.902Z" itemprop="datePublished">2017-03-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Language-Model"><a href="#Language-Model" class="headerlink" title="Language Model"></a>Language Model</h3><p>to predict the probability of observing the sentence (in a given dataset) as:</p>
<p>\begin{aligned}  P(w_1,…,w<em>m) = \prod</em>{i=1}^{m} P(w_i \mid w<em>1,…, w</em>{i-1})  \end{aligned}</p>
<p>计算一个句子出现概率有什么作用s</p>
<ul>
<li>翻译</li>
<li>语音识别</li>
<li>generative model - Because we can predict the probability of a word given the preceding words, we are able to generate new text</li>
</ul>
<p>缺点：当前置单词很多时，RNN遇到困难，引出LSTM。</p>
<h3 id="训练数据及其预处理"><a href="#训练数据及其预处理" class="headerlink" title="训练数据及其预处理"></a>训练数据及其预处理</h3><p>15000条评论数据</p>
<h4 id="分词-tokenize-text"><a href="#分词-tokenize-text" class="headerlink" title="分词  tokenize text"></a>分词  tokenize text</h4><p>借助NLTK中的word_tokenize和sent_tokenize方法</p>
<h4 id="去除低频词"><a href="#去除低频词" class="headerlink" title="去除低频词"></a>去除低频词</h4><p>原因：</p>
<ul>
<li>太多的词，模型难以训练</li>
<li>低频词没有足够的上下文样本数据</li>
</ul>
<p>低频词的处理方式：<br>按词频选取前N个（如8000），其它即为低频词，用“UNKNOWN_TOKEN”代替。生产新文本后，随机取词汇库外的一个词替换“UNKNOWN_TOKEN”，或者不断生产新文本，直到生成文本中不包含“UNKNOWN_TOKEN”为止。</p>
<h3 id="梯度消失问题-THE-VANISHING-GRADIENT-PROBLEM"><a href="#梯度消失问题-THE-VANISHING-GRADIENT-PROBLEM" class="headerlink" title="梯度消失问题 THE VANISHING GRADIENT PROBLEM"></a>梯度消失问题 THE VANISHING GRADIENT PROBLEM</h3><ul>
<li>W矩阵初始化</li>
<li>正则化 用ReLU代替tanh或 sigmoid激活函数】</li>
<li>采用LSTM或GRU(Gated Recurrent Unit, LSTM的简化版本)结构</li>
</ul>
<h2 id="Part4"><a href="#Part4" class="headerlink" title="Part4"></a>Part4</h2><h3 id="LSTM网络"><a href="#LSTM网络" class="headerlink" title="LSTM网络"></a>LSTM网络</h3><p>通过gating mechanism机制解决RNNs的梯度消失问题。<br>plain RNNs可以看做是LSTMs的特殊形式(input gate 取1，forget gate 取0，output gate 取1)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/03/09/A05-MI/10-DL理论/DL之05-RNN/" data-id="cj2egklrt001osco0qf9902uj" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-A05-MI/10-DL理论/LSTM" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/03/01/A05-MI/10-DL理论/LSTM/" class="article-date">
  <time datetime="2017-03-01T03:45:57.393Z" itemprop="datePublished">2017-03-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>window选择:</p>
<ul>
<li>固定时间跨度</li>
<li>固定数据长度，数据个数</li>
<li>window内的数据个数 + 数据方差</li>
</ul>
<h3 id="LSTM的应用"><a href="#LSTM的应用" class="headerlink" title="LSTM的应用"></a>LSTM的应用</h3><h4 id="序列分类"><a href="#序列分类" class="headerlink" title="序列分类"></a>序列分类</h4><ul>
<li><a href="http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/" target="_blank" rel="external">http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/</a><br>预测随空间或时间变化的序列的类别</li>
</ul>
<p>电影评论情感分类</p>
<p>word embedding</p>
<p>One-hot Representation<br>把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。</p>
<p>缺点：<br>维数灾难[Bengio 2003]<br>“词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系，哪怕是话筒和麦克这样的同义词也不能幸免于难。</p>
<p>Distributed representation<br> 最大的贡献就是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用 cos 夹角来衡量。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/03/01/A05-MI/10-DL理论/LSTM/" data-id="cj2egklrt001rsco0h9w9u31d" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-abc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/02/28/abc/" class="article-date">
  <time datetime="2017-02-28T04:17:00.000Z" itemprop="datePublished">2017-02-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/wiki/categories/MachineLearning/">MachineLearning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/wiki/2017/02/28/abc/">ML之01-核密度估计Kernel Density Estimation(KDE)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="ML之01-核密度估计Kernel-Density-Estimation-KDE"><a href="#ML之01-核密度估计Kernel-Density-Estimation-KDE" class="headerlink" title="ML之01-核密度估计Kernel Density Estimation(KDE)"></a>ML之01-核密度估计Kernel Density Estimation(KDE)</h2><hr>
<p>由给定样本集合求解随机变量的分布密度函数问题是概率统计学的基本问题之一。解决这一问题的方法包括参数估计和非参数估计。</p>
<h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><ul>
<li>参数回归分析<br>需要假定数据分布符合某种特定的性态，如线性、可化线性或指数性态等，然后在目标函数族中寻找特定的解，即确定回归模型中的未知参数。</li>
<li>参数判别分析<br>需要假定作为判别依据的、随机取值的数据样本在各个可能的类别中都服从特定的分布。</li>
</ul>
<p>经验和理论说明，参数模型的这种基本假定与实际的物理模型之间常常存在较大的差距，这些方法并非总能取得令人满意的结果。</p>
<h3 id="非参数估计"><a href="#非参数估计" class="headerlink" title="非参数估计"></a>非参数估计</h3><p>由于上述缺陷，Rosenblatt和Parzen提出了非参数估计方法，即核密度估计方法。核密度估计Kernel Density Estimation(KDE)是在概率论中用来估计未知的密度函数，属于非参数检验方法之一。</p>
<p>由于核密度估计方法不利用有关数据分布的先验知识，对数据分布不附加任何假定，是一种从数据样本本身出发研究数据分布特征的方法，因而，在统计学理论和应用领域均受到高度的重视。</p>
<p>核密度估计，就是采用平滑的峰值函数(“核”)来拟合观察到的数据点，从而对真实的概率分布曲线进行模拟。</p>
<h3 id="sklearn中实现的核函数"><a href="#sklearn中实现的核函数" class="headerlink" title="sklearn中实现的核函数"></a>sklearn中实现的核函数</h3><p>核密度估计有多种内核，如下图：<br><img src="../../../image/A0511/kde_kernels.png" alt="image"><br>sklearn核函数形式<br><img src="../../../image/A0511/sklearn_kernels.png" alt="image"><br>虽然采用不同的核函数都可以获得一致性的结论（整体趋势和密度分布规律性基本一致），但核密度函数也不是完美的。除了核算法的选择外，带宽（bandwidth）也会影响密度估计，过大或过小的带宽值都会影响估计结果。如下图(d,e,f)所示:<br><img src="../../../image/A0511/kde_bandwidth.png" alt="image"></p>
<h3 id="SVM的核函数如何选取"><a href="#SVM的核函数如何选取" class="headerlink" title="SVM的核函数如何选取"></a>SVM的核函数如何选取</h3><ol>
<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>
<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</li>
<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li>
</ol>
<h3 id="核密度估计的应用"><a href="#核密度估计的应用" class="headerlink" title="核密度估计的应用"></a>核密度估计的应用</h3><ul>
<li>股票、金融等风险预测<br>在单变量核密度估计的基础上，可以建立风险价值的预测模型。通过对核密度估计变异系数的加权处理，可以建立不同的风险价值的预测模型。</li>
<li>测量建筑密度</li>
<li>获取犯罪情况报告</li>
<li>发现对城镇或野生动物栖息地造成影响的道路或公共设施管线</li>
<li>热力图</li>
</ul>
<p>总而言之，核密度就是用来估计密度的，如果你有一系列空间点数据，那么核密度估计往往是比较好的可视化方法。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/02/28/abc/" data-id="cj2egklrt001ssco04sffyrz3" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/MachineLearning/">MachineLearning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-abc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/02/23/abc/" class="article-date">
  <time datetime="2017-02-23T00:47:00.000Z" itemprop="datePublished">2017-02-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/wiki/categories/DeepLearning/">DeepLearning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/wiki/2017/02/23/abc/">DL之02-深度学习中的Data Augmentation方法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="DL之02-深度学习中的Data-Augmentation方法"><a href="#DL之02-深度学习中的Data-Augmentation方法" class="headerlink" title="DL之02-深度学习中的Data Augmentation方法"></a>DL之02-深度学习中的Data Augmentation方法</h2><hr>
<p>在深度学习中，为了避免出现过拟合（Overfitting），通常我们需要输入充足的数据量。当数据量不够大时候，常常采用以下几种方法：</p>
<ol>
<li><p>Data Augmentation：通过平移、 翻转、加噪声等方法从已有数据中创造出一批“新”的数据，人工增加训练集的大小。</p>
</li>
<li><p>Regularization：数据量比较小会导致模型过拟合, 使得训练误差很小而测试误差特别大. 通过在Loss Function 后面加上正则项可以抑制过拟合的产生。缺点是引入了一个需要手动调整的hyper-parameter。</p>
</li>
<li><p>Dropout：这也是一种正则化手段，不过跟以上不同的是它通过随机将部分神经元的输出置零来实现。详见 <a href="http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank" rel="external">http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a></p>
</li>
<li><p>Unsupervised Pre-training：用Auto-Encoder或者RBM的卷积形式一层一层地做无监督预训练, 最后加上分类层做有监督的Fine-Tuning。参考 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.207.1102&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.207.1102&amp;rep=rep1&amp;type=pdf</a></p>
</li>
<li><p>Transfer Learning：在某些情况下，训练集的收集可能非常困难或代价高昂。因此，有必要创造出某种高性能学习机（learner），使得它们能够基于从其他领域易于获得的数据上进行训练，并能够在对另一领域的数据进行预测时表现优异。这种方法，就是所谓的迁移学习（transfer learning）。</p>
</li>
</ol>
<h3 id="数据增强变换（Data-Augmentation-Transformation）"><a href="#数据增强变换（Data-Augmentation-Transformation）" class="headerlink" title="数据增强变换（Data Augmentation Transformation）"></a>数据增强变换（Data Augmentation Transformation）</h3><h4 id="数字图像处理中图像几何变换方法："><a href="#数字图像处理中图像几何变换方法：" class="headerlink" title="数字图像处理中图像几何变换方法："></a>数字图像处理中图像几何变换方法：</h4><ul>
<li>旋转 | 反射变换(Rotation/reflection): 随机旋转图像一定角度; 改变图像内容的朝向;</li>
<li>翻转变换(flip): 沿着水平或者垂直方向翻转图像;</li>
<li>缩放变换(zoom): 按照一定的比例放大或者缩小图像;</li>
<li>平移变换(shift): 在图像平面上对图像以一定方式进行平移;可以采用随机或人为定义的方式指定平移范围和平移步长, 沿水平或竖直方向进行平移. 改变图像内容的位置;</li>
<li>尺度变换(scale): 对图像按照指定的尺度因子, 进行放大或缩小; 或者参照SIFT特征提取思想, 利用指定的尺度因子对图像滤波构造尺度空间. 改变图像内容的大小或模糊程度;</li>
<li>对比度变换(contrast): 在图像的HSV颜色空间，改变饱和度S和V亮度分量，保持色调H不变. 对每个像素的S和V分量进行指数运算(指数因子在0.25到4之间), 增加光照变化;</li>
<li>噪声扰动(noise): 对图像的每个像素RGB进行随机扰动, 常用的噪声模式是椒盐噪声和高斯噪声;</li>
<li>颜色变换(color): 在训练集像素值的RGB颜色空间进行PCA。</li>
</ul>
<p>不同的任务背景下, 我们可以通过图像的几何变换, 使用以下一种或多种组合数据增强变换来增加输入数据的量。 几何变换不改变像素值, 而是改变像素所在的位置。 通过Data Augmentation方法扩张了数据集的范围, 作为输入时, 以期待网络学习到更多的图像不变性特征。</p>
<h4 id="Keras中的图像几何变换方法"><a href="#Keras中的图像几何变换方法" class="headerlink" title="Keras中的图像几何变换方法"></a><a href="https://keras.io/preprocessing/image/" target="_blank" rel="external">Keras中的图像几何变换方法</a></h4><p>Keras中ImageDataGenerator　实现了大多数上文中提到的图像几何变换方法。如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">keras.preprocessing.image.ImageDataGenerator(featurewise_center=<span class="keyword">False</span>,</div><div class="line">    samplewise_center=<span class="keyword">False</span>,</div><div class="line">    featurewise_std_normalization=<span class="keyword">False</span>,</div><div class="line">    samplewise_std_normalization=<span class="keyword">False</span>,</div><div class="line">    zca_whitening=<span class="keyword">False</span>,</div><div class="line">    rotation_range=<span class="number">0.</span>,</div><div class="line">    width_shift_range=<span class="number">0.</span>,</div><div class="line">    height_shift_range=<span class="number">0.</span>,</div><div class="line">    shear_range=<span class="number">0.</span>,</div><div class="line">    zoom_range=<span class="number">0.</span>,</div><div class="line">    channel_shift_range=<span class="number">0.</span>,</div><div class="line">    fill_mode=<span class="string">'nearest'</span>,</div><div class="line">    cval=<span class="number">0.</span>,</div><div class="line">    horizontal_flip=<span class="keyword">False</span>,</div><div class="line">    vertical_flip=<span class="keyword">False</span>,</div><div class="line">    rescale=<span class="keyword">None</span>,</div><div class="line">    dim_ordering=K.image_dim_ordering())</div></pre></td></tr></table></figure>
<h5 id="参数说明："><a href="#参数说明：" class="headerlink" title="参数说明："></a>参数说明：</h5><ul>
<li>featurewise_center: Boolean. Set input mean to 0 over the dataset, feature-wise.</li>
<li>featurewise_std_normalization: Boolean. Divide inputs by std of the dataset, feature-wise.</li>
<li>zca_whitening: Boolean. Apply ZCA whitening.</li>
<li>samplewise_std_normalization: Boolean. Divide each input by its std.</li>
<li>width_shift_range: Float (fraction of total width). Range for random horizontal shifts.</li>
<li>rotation_range: Int. Degree range for random rotations.</li>
<li>height_shift_range: Float (fraction of total height). Range for random vertical shifts.</li>
<li>shear_range: Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)</li>
<li>zoom_range: Float or [lower, upper]. Range for random zoom. If a float,  [lower, upper] = [1-zoom_range, 1+zoom_range].</li>
<li>channel_shift_range: Float. Range for random channel shifts.</li>
<li>fill_mode: One of {“constant”, “nearest”, “reflect” or “wrap”}. Points outside the boundaries of the input are filled according to the given mode.</li>
<li>cval: Float or Int. Value used for points outside the boundaries when fill_mode = “constant”.</li>
<li>horizontal_flip: Boolean. Randomly flip inputs horizontally.</li>
<li>vertical_flip: Boolean. Randomly flip inputs vertically.</li>
<li>rescale: rescaling factor. Defaults to None. If None or 0, no rescaling is applied, otherwise we multiply the data by the value provided (before applying any other transformation).</li>
<li>dim_ordering: One of {“th”, “tf”}. “tf” mode means that the images should have shape  (samples, height, width, channels), “th” mode means that the images should have shape  (samples, channels, height, width). It defaults to the image_dim_ordering value found in your Keras config file at  ~/.keras/keras.json. If you never set it, then it will be “tf”.</li>
</ul>
<h5 id="其它方法"><a href="#其它方法" class="headerlink" title="其它方法"></a>其它方法</h5><ul>
<li>Label shuffle: 类别不平衡数据的增广，参见海康威视ILSVRC2016的report</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/02/23/abc/" data-id="cj2egkls9001vsco00dw05ccq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/MI/">MI</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-abc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/02/21/abc/" class="article-date">
  <time datetime="2017-02-21T11:58:00.000Z" itemprop="datePublished">2017-02-21</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/wiki/categories/DeepLearning/">DeepLearning</a>►<a class="article-category-link" href="/wiki/categories/DeepLearning/Keras/">Keras</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/wiki/2017/02/21/abc/">Keras之04-用Cifar10数据集训练一个CNN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Keras之04-用Cifar10数据集训练一个CNN"><a href="#Keras之04-用Cifar10数据集训练一个CNN" class="headerlink" title="Keras之04-用Cifar10数据集训练一个CNN"></a>Keras之04-用Cifar10数据集训练一个CNN</h2><hr>
<h3 id="Cifar数据集介绍"><a href="#Cifar数据集介绍" class="headerlink" title="Cifar数据集介绍"></a>Cifar数据集介绍</h3><p>Cifar是加拿大政府牵头投资的一个先进科学项目研究所。Cifar-10是由Hinton的两个大弟子Alex Krizhevsky、Ilya Sutskever收集的一个用于普适物体识别的数据集。</p>
<p>Cifar-10由60000张32*32的RGB彩色图片构成，共10个分类。50000张训练，10000张测试（交叉验证）。这个数据集最大的特点在于将识别迁移到了普适物体，而且应用于多分类（姊妹数据集Cifar-100达到100类，ILSVRC比赛则是1000类）。<br><img src="../../../image/A0501/Cifar-dataset.png" alt="image"></p>
<h4 id="普适物体识别的挑战"><a href="#普适物体识别的挑战" class="headerlink" title="普适物体识别的挑战"></a>普适物体识别的挑战</h4><ul>
<li>数据中含有大量特征、噪声</li>
<li>识别物体比例不一</li>
<li>分类庞大（SVM难以应对）</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/02/21/abc/" data-id="cj2egklrt001nsco03od6cxp8" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Keras/">Keras</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/MI/">MI</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-abc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/01/19/abc/" class="article-date">
  <time datetime="2017-01-19T10:23:00.000Z" itemprop="datePublished">2017-01-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/wiki/categories/DeepLearning/">DeepLearning</a>►<a class="article-category-link" href="/wiki/categories/DeepLearning/Keras/">Keras</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/wiki/2017/01/19/abc/">Keras之03-用MNIST数据集训练一个CNN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Keras之03-用MNIST数据集训练一个CNN"><a href="#Keras之03-用MNIST数据集训练一个CNN" class="headerlink" title="Keras之03-用MNIST数据集训练一个CNN"></a>Keras之03-用MNIST数据集训练一个CNN</h2><hr>
<h4 id="模型code"><a href="#模型code" class="headerlink" title="模型code"></a>模型code</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="string">'''Trains a simple convnet on the MNIST dataset.</span></div><div class="line"></div><div class="line">Gets to 99.25% test accuracy after 12 epochs</div><div class="line">(there is still a lot of margin for parameter tuning).</div><div class="line">16 seconds per epoch on a GRID K520 GPU.</div><div class="line">'''</div><div class="line"></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout, Activation, Flatten</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Convolution2D, MaxPooling2D</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div><div class="line"><span class="comment"># Keras的底层库使用Theano或TensorFlow</span></div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"></div><div class="line">batch_size = <span class="number">128</span></div><div class="line">nb_classes = <span class="number">10</span></div><div class="line">nb_epoch = <span class="number">12</span></div><div class="line"></div><div class="line"><span class="comment"># input image dimensions</span></div><div class="line">img_rows, img_cols = <span class="number">28</span>, <span class="number">28</span></div><div class="line"><span class="comment"># number of convolutional filters to use</span></div><div class="line">nb_filters = <span class="number">32</span></div><div class="line"><span class="comment"># size of pooling area for max pooling</span></div><div class="line">pool_size = (<span class="number">2</span>, <span class="number">2</span>)</div><div class="line"><span class="comment"># convolution kernel size</span></div><div class="line">kernel_size = (<span class="number">3</span>, <span class="number">3</span>)</div><div class="line"></div><div class="line"><span class="comment"># the data, shuffled and split between train and test sets</span></div><div class="line">(X_train, y_train), (X_test, y_test) = mnist.load_data()</div><div class="line"></div><div class="line"><span class="comment"># 在如何表示一组彩色图片的问题上，Theano和TensorFlow发生了分歧.</span></div><div class="line"><span class="comment"># ’th’模式，也即Theano模式会把100张RGB三通道的16×32（高为16宽为32）彩色图表示为下面这种形式（100,3,16,32），Caffe采取的也是这种方式。第0个维度是样本维，代表样本的数目，第1个维度是通道维，代表颜色通道数。后面两个就是高和宽了。</span></div><div class="line"><span class="comment"># 而TensorFlow，即’tf’模式的表达形式是（100,16,32,3），即把通道维放在了最后。</span></div><div class="line"></div><div class="line"><span class="comment"># 根据backend模式reshape输入数据</span></div><div class="line"><span class="keyword">if</span> K.image_dim_ordering() == <span class="string">'th'</span>:</div><div class="line">    X_train = X_train.reshape(X_train.shape[<span class="number">0</span>], <span class="number">1</span>, img_rows, img_cols)</div><div class="line">    X_test = X_test.reshape(X_test.shape[<span class="number">0</span>], <span class="number">1</span>, img_rows, img_cols)</div><div class="line">    input_shape = (<span class="number">1</span>, img_rows, img_cols)</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    X_train = X_train.reshape(X_train.shape[<span class="number">0</span>], img_rows, img_cols, <span class="number">1</span>)</div><div class="line">    X_test = X_test.reshape(X_test.shape[<span class="number">0</span>], img_rows, img_cols, <span class="number">1</span>)</div><div class="line">    input_shape = (img_rows, img_cols, <span class="number">1</span>)</div><div class="line"></div><div class="line">X_train = X_train.astype(<span class="string">'float32'</span>)</div><div class="line">X_test = X_test.astype(<span class="string">'float32'</span>)</div><div class="line">X_train /= <span class="number">255</span></div><div class="line">X_test /= <span class="number">255</span></div><div class="line">print(<span class="string">'X_train shape:'</span>, X_train.shape)</div><div class="line">print(X_train.shape[<span class="number">0</span>], <span class="string">'train samples'</span>)</div><div class="line">print(X_test.shape[<span class="number">0</span>], <span class="string">'test samples'</span>)</div><div class="line"></div><div class="line"><span class="comment"># convert class vectors to binary class matrices</span></div><div class="line">Y_train = np_utils.to_categorical(y_train, nb_classes)</div><div class="line">Y_test = np_utils.to_categorical(y_test, nb_classes)</div><div class="line"></div><div class="line">model = Sequential()</div><div class="line"></div><div class="line"><span class="comment"># 卷积层    </span></div><div class="line"><span class="comment"># 二维卷积层对二维输入进行滑动窗卷积</span></div><div class="line"><span class="comment"># keras.layers.convolutional.Convolution2D(nb_filter, nb_row, nb_col, init='glorot_uniform', activation='linear', weights=None, border_mode='valid', subsample=(1, 1), dim_ordering='th', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)</span></div><div class="line"></div><div class="line"><span class="comment"># nb_filter：卷积核的数目,（即输出的维度）</span></div><div class="line"><span class="comment"># nb_row：卷积核的行数</span></div><div class="line"><span class="comment"># nb_col：卷积核的列数</span></div><div class="line"><span class="comment"># border_mode：边界模式，为“valid”，“same”或“full”，full需要以theano为后端</span></div><div class="line"></div><div class="line">model.add(Convolution2D(nb_filters, kernel_size[<span class="number">0</span>], kernel_size[<span class="number">1</span>],</div><div class="line">                        border_mode=<span class="string">'valid'</span>,</div><div class="line">                        input_shape=input_shape))</div><div class="line">model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">model.add(Convolution2D(nb_filters, kernel_size[<span class="number">0</span>], kernel_size[<span class="number">1</span>]))</div><div class="line">model.add(Activation(<span class="string">'relu'</span>))</div><div class="line"></div><div class="line"><span class="comment"># keras.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th')</span></div><div class="line"><span class="comment"># 空域信号施加最大值池化</span></div><div class="line">model.add(MaxPooling2D(pool_size=pool_size))</div><div class="line">model.add(Dropout(<span class="number">0.25</span>))</div><div class="line"></div><div class="line"><span class="comment"># Flatten层用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten不影响batch的大小。</span></div><div class="line">model.add(Flatten())</div><div class="line">model.add(Dense(<span class="number">128</span>))</div><div class="line">model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">model.add(Dropout(<span class="number">0.5</span>))</div><div class="line">model.add(Dense(nb_classes))</div><div class="line">model.add(Activation(<span class="string">'softmax'</span>))</div><div class="line"></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</div><div class="line">              optimizer=<span class="string">'adadelta'</span>,</div><div class="line">              metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line">model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,</div><div class="line">          verbose=<span class="number">1</span>, validation_data=(X_test, Y_test))</div><div class="line">score = model.evaluate(X_test, Y_test, verbose=<span class="number">0</span>)</div><div class="line">print(<span class="string">'Test score:'</span>, score[<span class="number">0</span>])</div><div class="line">print(<span class="string">'Test accuracy:'</span>, score[<span class="number">1</span>])</div></pre></td></tr></table></figure>
<h4 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">Using TensorFlow backend.</div><div class="line">X_train shape: (60000, 28, 28, 1)</div><div class="line">60000 train samples</div><div class="line">10000 <span class="built_in">test</span> samples</div><div class="line">Train on 60000 samples, validate on 10000 samples</div><div class="line">Epoch 1/12</div><div class="line">60000/60000 [==============================] - 46s - loss: 0.3732 - acc: 0.8859 - val_loss: 0.0886 - val_acc: 0.9719</div><div class="line">Epoch 2/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.1350 - acc: 0.9597 - val_loss: 0.0627 - val_acc: 0.9796</div><div class="line">Epoch 3/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.1027 - acc: 0.9697 - val_loss: 0.0562 - val_acc: 0.9822</div><div class="line">Epoch 4/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0884 - acc: 0.9741 - val_loss: 0.0438 - val_acc: 0.9858</div><div class="line">Epoch 5/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0779 - acc: 0.9772 - val_loss: 0.0415 - val_acc: 0.9867</div><div class="line">Epoch 6/12</div><div class="line">60000/60000 [==============================] - 46s - loss: 0.0709 - acc: 0.9786 - val_loss: 0.0379 - val_acc: 0.9869</div><div class="line">Epoch 7/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0650 - acc: 0.9811 - val_loss: 0.0360 - val_acc: 0.9889</div><div class="line">Epoch 8/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0609 - acc: 0.9813 - val_loss: 0.0354 - val_acc: 0.9883</div><div class="line">Epoch 9/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0557 - acc: 0.9838 - val_loss: 0.0330 - val_acc: 0.9885</div><div class="line">Epoch 10/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0541 - acc: 0.9836 - val_loss: 0.0318 - val_acc: 0.9897</div><div class="line">Epoch 11/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0497 - acc: 0.9857 - val_loss: 0.0322 - val_acc: 0.9897</div><div class="line">Epoch 12/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0476 - acc: 0.9856 - val_loss: 0.0327 - val_acc: 0.9893</div><div class="line">Test score: 0.0326897691154</div><div class="line">Test accuracy: 0.9893</div></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/01/19/abc/" data-id="cj2egklrt001msco007eb12gu" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Keras/">Keras</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/MI/">MI</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-abc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/01/19/abc/" class="article-date">
  <time datetime="2017-01-19T08:30:00.000Z" itemprop="datePublished">2017-01-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/wiki/categories/DeepLearning/">DeepLearning</a>►<a class="article-category-link" href="/wiki/categories/DeepLearning/Keras/">Keras</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/wiki/2017/01/19/abc/">Keras之01-用MNIST数据集训练一个DNN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Keras之01-用MNIST数据集训练一个DNN"><a href="#Keras之01-用MNIST数据集训练一个DNN" class="headerlink" title="Keras之01-用MNIST数据集训练一个DNN"></a>Keras之01-用MNIST数据集训练一个DNN</h2><hr>
<h4 id="模型code"><a href="#模型code" class="headerlink" title="模型code"></a>模型code</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="string">'''Trains a simple deep NN on the MNIST dataset.</span></div><div class="line"></div><div class="line">Gets to 98.40% test accuracy after 20 epochs</div><div class="line">(there is *a lot* of margin for parameter tuning).</div><div class="line">2 seconds per epoch on a K520 GPU.</div><div class="line">'''</div><div class="line"></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Dropout, Activation</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div><div class="line"></div><div class="line"></div><div class="line">batch_size = <span class="number">128</span></div><div class="line">nb_classes = <span class="number">10</span></div><div class="line">nb_epoch = <span class="number">20</span></div><div class="line"></div><div class="line"><span class="comment"># the data, shuffled and split between train and test sets</span></div><div class="line">(X_train, y_train), (X_test, y_test) = mnist.load_data()</div><div class="line"></div><div class="line"><span class="comment"># 训练数据 60000张手写图片，28*28*1</span></div><div class="line"><span class="comment"># 测试数据 10000张手写图片，28*28*1</span></div><div class="line"></div><div class="line">X_train = X_train.reshape(<span class="number">60000</span>, <span class="number">784</span>)</div><div class="line">X_test = X_test.reshape(<span class="number">10000</span>, <span class="number">784</span>)</div><div class="line">X_train = X_train.astype(<span class="string">'float32'</span>)</div><div class="line">X_test = X_test.astype(<span class="string">'float32'</span>)</div><div class="line"><span class="comment"># 归一化到0-1</span></div><div class="line">X_train /= <span class="number">255</span></div><div class="line">X_test /= <span class="number">255</span></div><div class="line">print(X_train.shape[<span class="number">0</span>], <span class="string">'train samples'</span>)</div><div class="line">print(X_test.shape[<span class="number">0</span>], <span class="string">'test samples'</span>)</div><div class="line"></div><div class="line"><span class="comment"># convert class vectors to binary class matrices</span></div><div class="line"><span class="comment"># to_categorical(y, nb_classes=None)</span></div><div class="line"><span class="comment"># 将类别向量(从0到nb_classes的整数向量)映射为二值类别矩阵, 用于应用到以categorical_crossentropy为目标函数的模型中.</span></div><div class="line"><span class="comment"># y: 类别向量; nb_classes:总共类别数</span></div><div class="line">Y_train = np_utils.to_categorical(y_train, nb_classes)</div><div class="line">Y_test = np_utils.to_categorical(y_test, nb_classes)</div><div class="line"></div><div class="line"><span class="comment"># Dense层:即全连接层</span></div><div class="line"><span class="comment"># keras.layers.core.Dense(output_dim, init='glorot_uniform', activation='linear', weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None)</span></div><div class="line"></div><div class="line"></div><div class="line">model = Sequential()</div><div class="line">model.add(Dense(<span class="number">512</span>, input_shape=(<span class="number">784</span>,)))</div><div class="line">model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">model.add(Dropout(<span class="number">0.2</span>))</div><div class="line"><span class="comment"># 激活函数可以通过设置单独的激活层实现，也可以在构造层对象时通过传递activation参数实现。</span></div><div class="line"><span class="comment"># 以下两行等价于：model.add(Dense(512,activation='relu'))</span></div><div class="line">model.add(Dense(<span class="number">512</span>))</div><div class="line">model.add(Activation(<span class="string">'relu'</span>))</div><div class="line"></div><div class="line"><span class="comment"># Dropout  需要断开的连接的比例</span></div><div class="line">model.add(Dropout(<span class="number">0.2</span>))</div><div class="line">model.add(Dense(<span class="number">10</span>))</div><div class="line">model.add(Activation(<span class="string">'softmax'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 打印出模型概况</span></div><div class="line">print(<span class="string">'model.summary:'</span>)</div><div class="line">model.summary()</div><div class="line"></div><div class="line"><span class="comment"># 在训练模型之前，通过compile来对学习过程进行配置</span></div><div class="line"><span class="comment"># 编译模型以供训练</span></div><div class="line"><span class="comment"># 包含评估模型在训练和测试时的性能的指标，典型用法是metrics=['accuracy']</span></div><div class="line"><span class="comment"># 如果要在多输出模型中为不同的输出指定不同的指标，可像该参数传递一个字典，例如metrics=&#123;'ouput_a': 'accuracy'&#125;</span></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</div><div class="line">              optimizer=RMSprop(),</div><div class="line">              metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line"><span class="comment"># 训练模型</span></div><div class="line"><span class="comment"># Keras以Numpy数组作为输入数据和标签的数据类型</span></div><div class="line"><span class="comment"># fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None)</span></div><div class="line"><span class="comment"># nb_epoch：整数，训练的轮数，训练数据将会被遍历nb_epoch次。Keras中nb开头的变量均为"number of"的意思</span></div><div class="line"><span class="comment"># verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录</span></div><div class="line"><span class="comment"># shuffle：布尔值，表示是否在训练过程中每个epoch前随机打乱输入样本的顺序。</span></div><div class="line"></div><div class="line"><span class="comment"># fit函数返回一个History的对象，其History.history属性记录了损失函数和其他指标的数值随epoch变化的情况，如果有验证集的话，也包含了验证集的这些指标变化情况</span></div><div class="line">history = model.fit(X_train, Y_train,</div><div class="line">                    batch_size=batch_size, nb_epoch=nb_epoch,</div><div class="line">                    verbose=<span class="number">1</span>, validation_data=(X_test, Y_test))</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)</span></div><div class="line"><span class="comment"># 按batch计算在某些输入数据上模型的误差</span></div><div class="line">print(<span class="string">'-------evaluate--------'</span>)</div><div class="line">score = model.evaluate(X_test, Y_test, verbose=<span class="number">1</span>)</div><div class="line">print(<span class="string">'Test score:'</span>, score[<span class="number">0</span>])</div><div class="line">print(<span class="string">'Test accuracy:'</span>, score[<span class="number">1</span>])</div></pre></td></tr></table></figure>
<h4 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line">Using TensorFlow backend.</div><div class="line">60000 train samples</div><div class="line">10000 <span class="built_in">test</span> samples</div><div class="line">model.summary:</div><div class="line">____________________________________________________________________________________________________</div><div class="line">Layer (<span class="built_in">type</span>)                     Output Shape          Param <span class="comment">#     Connected to                     </span></div><div class="line">====================================================================================================</div><div class="line">dense_1 (Dense)                  (None, 512)           401920      dense_input_1[0][0]              </div><div class="line">____________________________________________________________________________________________________</div><div class="line">activation_1 (Activation)        (None, 512)           0           dense_1[0][0]                    </div><div class="line">____________________________________________________________________________________________________</div><div class="line">dropout_1 (Dropout)              (None, 512)           0           activation_1[0][0]               </div><div class="line">____________________________________________________________________________________________________</div><div class="line">dense_2 (Dense)                  (None, 512)           262656      dropout_1[0][0]                  </div><div class="line">____________________________________________________________________________________________________</div><div class="line">activation_2 (Activation)        (None, 512)           0           dense_2[0][0]                    </div><div class="line">____________________________________________________________________________________________________</div><div class="line">dropout_2 (Dropout)              (None, 512)           0           activation_2[0][0]               </div><div class="line">____________________________________________________________________________________________________</div><div class="line">dense_3 (Dense)                  (None, 10)            5130        dropout_2[0][0]                  </div><div class="line">____________________________________________________________________________________________________</div><div class="line">activation_3 (Activation)        (None, 10)            0           dense_3[0][0]                    </div><div class="line">====================================================================================================</div><div class="line">Total params: 669,706</div><div class="line">Trainable params: 669,706</div><div class="line">Non-trainable params: 0</div><div class="line">____________________________________________________________________________________________________</div><div class="line">Train on 60000 samples, validate on 10000 samples</div><div class="line">Epoch 1/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.2444 - acc: 0.9243 - val_loss: 0.1180 - val_acc: 0.9642</div><div class="line">Epoch 2/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.1009 - acc: 0.9691 - val_loss: 0.0810 - val_acc: 0.9756</div><div class="line">Epoch 3/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0746 - acc: 0.9771 - val_loss: 0.0782 - val_acc: 0.9767</div><div class="line">Epoch 4/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0590 - acc: 0.9825 - val_loss: 0.0783 - val_acc: 0.9774</div><div class="line">Epoch 5/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0513 - acc: 0.9847 - val_loss: 0.0823 - val_acc: 0.9792</div><div class="line">Epoch 6/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0453 - acc: 0.9867 - val_loss: 0.0769 - val_acc: 0.9812</div><div class="line">Epoch 7/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0380 - acc: 0.9887 - val_loss: 0.0756 - val_acc: 0.9812</div><div class="line">Epoch 8/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0341 - acc: 0.9901 - val_loss: 0.0771 - val_acc: 0.9827</div><div class="line">Epoch 9/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0321 - acc: 0.9907 - val_loss: 0.0900 - val_acc: 0.9809</div><div class="line">Epoch 10/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0325 - acc: 0.9915 - val_loss: 0.0875 - val_acc: 0.9818</div><div class="line">Epoch 11/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0285 - acc: 0.9917 - val_loss: 0.0849 - val_acc: 0.9837</div><div class="line">Epoch 12/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0261 - acc: 0.9925 - val_loss: 0.0886 - val_acc: 0.9835</div><div class="line">Epoch 13/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0240 - acc: 0.9930 - val_loss: 0.1016 - val_acc: 0.9810</div><div class="line">Epoch 14/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0244 - acc: 0.9936 - val_loss: 0.0956 - val_acc: 0.9826</div><div class="line">Epoch 15/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0194 - acc: 0.9944 - val_loss: 0.0950 - val_acc: 0.9843</div><div class="line">Epoch 16/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0219 - acc: 0.9943 - val_loss: 0.1143 - val_acc: 0.9810</div><div class="line">Epoch 17/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0197 - acc: 0.9944 - val_loss: 0.1056 - val_acc: 0.9841</div><div class="line">Epoch 18/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0212 - acc: 0.9948 - val_loss: 0.1143 - val_acc: 0.9833</div><div class="line">Epoch 19/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0202 - acc: 0.9951 - val_loss: 0.1056 - val_acc: 0.9835</div><div class="line">Epoch 20/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0188 - acc: 0.9954 - val_loss: 0.1045 - val_acc: 0.9847</div><div class="line">-------evaluate--------</div><div class="line"> 9952/10000 [============================&gt;.] - ETA: 0sTest score: 0.104524913335</div><div class="line">Test accuracy: 0.9847</div></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/01/19/abc/" data-id="cj2egklrd001jsco00wb2iimq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Keras/">Keras</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/MI/">MI</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-abc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/01/19/abc/" class="article-date">
  <time datetime="2017-01-19T02:46:00.000Z" itemprop="datePublished">2017-01-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/wiki/categories/DeepLearning/">DeepLearning</a>►<a class="article-category-link" href="/wiki/categories/DeepLearning/Keras/">Keras</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/wiki/2017/01/19/abc/">Keras之00-入门介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Keras之00-入门介绍"><a href="#Keras之00-入门介绍" class="headerlink" title="Keras之00-入门介绍"></a>Keras之00-入门介绍</h2><hr>
<p>Keras是一个高层神经网络库，Keras由纯Python编写而成并基于Tensorflow或Theano。Keras 为支持快速实验而生，能够把你的idea迅速转换为结果。</p>
<h3 id="Keras特点"><a href="#Keras特点" class="headerlink" title="Keras特点"></a>Keras特点</h3><ul>
<li>简易和快速的原型设计（keras具有高度模块化，极简，和可扩充特性）</li>
<li>支持CNN和RNN，或二者的结合</li>
<li>支持任意的链接方案（包括多输入和多输出训练）</li>
<li>无缝CPU和GPU切换</li>
</ul>
<h3 id="Keras设计原则"><a href="#Keras设计原则" class="headerlink" title="Keras设计原则"></a>Keras设计原则</h3><ul>
<li>模块性：模型可理解为一个独立的序列或图，完全可配置的模块以最少的代价自由组合在一起。具体而言，网络层、损失函数、优化器、初始化策略、激活函数、正则化方法都是独立的模块，你可以使用它们来构建自己的模型。</li>
<li>极简主义：每个模块都应该尽量的简洁。每一段代码都应该在初次阅读时都显得直观易懂。没有黑魔法，因为它将给迭代和创新带来麻烦。</li>
<li>易扩展性：添加新模块超级简单的容易，只需要仿照现有的模块编写新的类或函数即可。创建新模块的便利性使得Keras更适合于先进的研究工作。</li>
<li>与Python协作：Keras没有单独的模型配置文件类型（作为对比，caffe有），模型由python代码描述，使其更紧凑和更易debug，并提供了扩展的便利性。</li>
</ul>
<h3 id="Keras新特性"><a href="#Keras新特性" class="headerlink" title="Keras新特性"></a>Keras新特性</h3><ul>
<li>泛型模型：简单和强大的新模块，用于支持复杂深度学习模型的搭建。</li>
<li>更优秀的性能：现在，Keras模型的编译时间得到缩短。所有的RNN现在都可以用两种方式实现，以供用户在不同配置任务和配置环境下取得最大性能。现在，基于Theano的RNN也可以被展开，以获得大概25%的加速计算。</li>
<li>测量指标：现在，你可以提供一系列的测量指标来在Keras的任何监测点观察模型性能。</li>
<li>更优的用户体验：我们面向使用者重新编写了代码，使得函数API更简单易记，同时提供更有效的出错信息。</li>
<li>新版本的Keras提供了Lambda层，以实现一些简单的计算任务。</li>
</ul>
<h3 id="Keras基本概念"><a href="#Keras基本概念" class="headerlink" title="Keras基本概念"></a>Keras基本概念</h3><h4 id="符号计算"><a href="#符号计算" class="headerlink" title="符号计算"></a>符号计算</h4><p>Keras的底层库使用Theano或TensorFlow，这两个库也称为Keras的后端。Theano和TensorFlow都是一个“符号主义”的库。</p>
<p>符号主义的计算首先定义各种变量，然后建立一个“计算图”，计算图规定了各个变量之间的计算关系。建立好的计算图需要编译已确定其内部细节，然而，此时的计算图还是一个“空壳子”，里面没有任何实际的数据，只有当你把需要运算的输入放进去后，才能在整个模型中形成数据流，从而形成输出值。</p>
<p>Keras的模型搭建形式就是这种方法，在你搭建Keras模型完毕后，你的模型就是一个空壳子，只有实际生成可调用的函数后（K.function），输入数据，才会形成真正的数据流。</p>
<p>大多数的深度学习框架使用的都是符号计算这一套方法，因为符号计算能够提供关键的计算优化、自动求导等功能。</p>
<h4 id="张量-tensor"><a href="#张量-tensor" class="headerlink" title="张量(tensor)"></a>张量(tensor)</h4><p>张量可以看作是向量、矩阵的自然推广，我们用张量来表示广泛的数据类型。</p>
<p>规模最小的张量是0阶张量，即标量，也就是一个数。</p>
<p>当我们把一些数有序的排列起来，就形成了1阶张量，也就是一个向量</p>
<p>如果我们继续把一组向量有序的排列起来，就形成了2阶张量，也就是一个矩阵</p>
<p>把矩阵摞起来，就是3阶张量，我们可以称为一个立方体，具有3个颜色通道的彩色图片就是一个这样的立方体</p>
<p>把矩阵摞起来，好吧这次我们真的没有给它起别名了，就叫4阶张量了，不要去试图想像4阶张量是什么样子，它就是个数学上的概念。</p>
<p>张量的阶数有时候也称为维度，或者轴，轴这个词翻译自英文axis。譬如一个矩阵[[1,2],[3,4]]，是一个2阶张量，有两个维度或轴，沿着第0个轴（为了与python的计数方式一致，本文档维度和轴从0算起）你看到的是[1,2]，[3,4]两个向量，沿着第1个轴你看到的是[1,3]，[2,4]两个向量。</p>
<p>要理解“沿着某个轴”是什么意思，不妨试着运行一下下面的代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</div><div class="line">sum0 = np.sum(a, axis=<span class="number">0</span>)</div><div class="line">sum1 = np.sum(a, axis=<span class="number">1</span>)</div><div class="line"></div><div class="line">print(sum0)  <span class="comment"># [4 6]</span></div><div class="line">print(sum1)  <span class="comment"># [3 7]</span></div></pre></td></tr></table></figure></p>
<h4 id="‘th’与’tf’"><a href="#‘th’与’tf’" class="headerlink" title="‘th’与’tf’"></a>‘th’与’tf’</h4><p>这是一个无可奈何的问题，在如何表示一组彩色图片的问题上，Theano和TensorFlow发生了分歧，’th’模式，也即Theano模式会把100张RGB三通道的16×32（高为16宽为32）彩色图表示为下面这种形式（100,3,16,32），Caffe采取的也是这种方式。第0个维度是样本维，代表样本的数目，第1个维度是通道维，代表颜色通道数。后面两个就是高和宽了。</p>
<p>而TensorFlow，即’tf’模式的表达形式是（100,16,32,3），即把通道维放在了最后。</p>
<p>Keras默认的数据组织形式在~/.keras/keras.json中规定，可查看该文件的image_dim_ordering一项查看，也可在代码中通过K.image_dim_ordering()函数返回，请在网络的训练和测试中保持维度顺序一致。</p>
<h4 id="泛型模型"><a href="#泛型模型" class="headerlink" title="泛型模型"></a>泛型模型</h4><p>Keras的核心数据结构是“模型”，模型是一种组织网络层的方式。Keras中主要的模型是Sequential模型，Sequential是一系列网络层按顺序构成的栈。</p>
<p>在原本的Keras版本中，模型其实有两种:</p>
<ul>
<li>Sequential，称为序贯模型，也就是单输入单输出，一条路通到底，层与层之间只有相邻关系，跨层连接统统没有。这种模型编译速度快，操作上也比较简单</li>
<li>Graph，即图模型，这个模型支持多输入多输出，层与层之间想怎么连怎么连，但是编译速度慢。Sequential其实是Graph的一个特殊情况。</li>
</ul>
<p>在现在这版Keras中，图模型被移除，而增加了了“functional model API”。由于functional model API表达的是“一般的模型”这个概念，我们这里将其译为泛型模型，即只要这个东西接收一个或一些张量作为输入，然后输出的也是一个或一些张量，那不管它是什么鬼，统统都称作“模型”。</p>
<h4 id="batch"><a href="#batch" class="headerlink" title="batch"></a>batch</h4><p>参数更新的方式方式：</p>
<ul>
<li><p>Batch gradient descent，批梯度下降<br>遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习。</p>
</li>
<li><p>stochastic gradient descent(SGD)，随机梯度下降<br>每看一个数据就算一下损失函数，然后求梯度更新参数。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。</p>
</li>
<li><p>mini-batch gradient decent，小批的梯度下降<br>为了克服两种方法的缺点，现在一般采用的是一种折中手段，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。</p>
</li>
</ul>
<p>基本上现在的梯度下降都是基于mini-batch的，所以Keras的模块中经常会出现batch_size，就是指这个。</p>
<p><strong>转载请注明作者Levine Huang及其出处</strong></p>
<p>Github博客主页(<a href="http://levinehuang.github.io/wiki/">http://levinehuang.github.io/wiki/</a>)<br>CSDN博客(<a href="http://blog.csdn.net/donhlj" target="_blank" rel="external">http://blog.csdn.net/donhlj</a>)<br>简书主页(<a href="http://www.jianshu.com/users/2bd9b48f6ea8/latest_articles" target="_blank" rel="external">http://www.jianshu.com/users/2bd9b48f6ea8/latest_articles</a>)<br><strong>百度搜索levinehuang进入我的博客主页</strong></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul>
<li><a href="http://keras-cn.readthedocs.io/en/latest/" target="_blank" rel="external">http://keras-cn.readthedocs.io/en/latest/</a>　</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/01/19/abc/" data-id="cj2egklrd001isco05q5nwpp9" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Keras/">Keras</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/MI/">MI</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-scala" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2016/12/07/scala/" class="article-date">
  <time datetime="2016-12-07T00:50:00.000Z" itemprop="datePublished">2016-12-07</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/wiki/categories/language/">language</a>►<a class="article-category-link" href="/wiki/categories/language/scala/">scala</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/wiki/2016/12/07/scala/">Scala高级类型</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>###　Scala类型系统总结<br>类型    语法类型<br> 类    class Person<br> 特质    trait Closable<br> 元组类型    (T1,T2,T3,…)<br> 函数类型    (T1,T2,t3,…)=&gt;T<br> 参数类型（泛型）    class Person[T1,T2,…]<br> 单例类型    this.type<br> 类型投影    Outter#Inner<br> 复合类型    A with B with C…<br> 结构体类型    {def f():Unit ….}<br> 中置类型    T1 A T2<br> 存在类型    T forSome {}</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>［１］ <a href="https://yq.aliyun.com/articles/60370?spm=5176.8251999.569296.24" target="_blank" rel="external">https://yq.aliyun.com/articles/60370?spm=5176.8251999.569296.24</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2016/12/07/scala/" data-id="cj2egklos0000sco0vgyhvgd8" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Scala/">Scala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Spark/">Spark</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/wiki/page/2/">2</a><a class="extend next" rel="next" href="/wiki/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/DeepLearning/">DeepLearning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/DeepLearning/Keras/">Keras</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/MachineLearning/">MachineLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/Tools/">Tools</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/Tools/hexo/">hexo</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/language/">language</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/language/scala/">scala</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/日志/">日志</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/日志/运维/">运维</a></li></ul></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/AI/">AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/Keras/">Keras</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/MI/">MI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/MachineLearning/">MachineLearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/Scala/">Scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/Spark/">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/Test/">Test</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/hexo/">hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/package/">package</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/pip/">pip</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/wiki/tags/AI/" style="font-size: 20px;">AI</a> <a href="/wiki/tags/Deep-Learning/" style="font-size: 20px;">Deep Learning</a> <a href="/wiki/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/wiki/tags/MI/" style="font-size: 20px;">MI</a> <a href="/wiki/tags/MachineLearning/" style="font-size: 10px;">MachineLearning</a> <a href="/wiki/tags/Python/" style="font-size: 10px;">Python</a> <a href="/wiki/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/wiki/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/wiki/tags/Test/" style="font-size: 10px;">Test</a> <a href="/wiki/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/wiki/tags/package/" style="font-size: 10px;">package</a> <a href="/wiki/tags/pip/" style="font-size: 10px;">pip</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/wiki/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/wiki/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/wiki/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/wiki/archives/2016/12/">December 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/wiki/archives/2016/11/">November 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/wiki/2017/03/22/A01-BigData/02-Spark/SparkSQL/">(no title)</a>
          </li>
        
          <li>
            <a href="/wiki/2017/03/09/A05-MI/10-DL理论/DL之05-RNN/">(no title)</a>
          </li>
        
          <li>
            <a href="/wiki/2017/03/01/A05-MI/10-DL理论/LSTM/">(no title)</a>
          </li>
        
          <li>
            <a href="/wiki/2017/02/28/abc/">ML之01-核密度估计Kernel Density Estimation(KDE)</a>
          </li>
        
          <li>
            <a href="/wiki/2017/02/23/abc/">DL之02-深度学习中的Data Augmentation方法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Levine Huang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/wiki/" class="mobile-nav-link">Home</a>
  
    <a href="/wiki/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/wiki/fancybox/jquery.fancybox.css">
  <script src="/wiki/fancybox/jquery.fancybox.pack.js"></script>


<script src="/wiki/js/script.js"></script>

  </div>
</body>
</html>