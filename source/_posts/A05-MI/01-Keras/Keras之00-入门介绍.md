---
title: Keras之00-入门介绍
date: 2017-01-19 10:46:00
updated	: 2017-01-19 10:46:00
permalink: abc
tags:
- Keras
- Deep Learning
- AI
- MI
categories:
- DeepLearning
- Keras
---

## Keras之00-入门介绍
----

Keras是一个高层神经网络库，Keras由纯Python编写而成并基于Tensorflow或Theano。Keras 为支持快速实验而生，能够把你的idea迅速转换为结果。

### Keras特点
+ 简易和快速的原型设计（keras具有高度模块化，极简，和可扩充特性）
+ 支持CNN和RNN，或二者的结合
+ 支持任意的链接方案（包括多输入和多输出训练）
+ 无缝CPU和GPU切换

### Keras设计原则
+ 模块性：模型可理解为一个独立的序列或图，完全可配置的模块以最少的代价自由组合在一起。具体而言，网络层、损失函数、优化器、初始化策略、激活函数、正则化方法都是独立的模块，你可以使用它们来构建自己的模型。
+ 极简主义：每个模块都应该尽量的简洁。每一段代码都应该在初次阅读时都显得直观易懂。没有黑魔法，因为它将给迭代和创新带来麻烦。
+ 易扩展性：添加新模块超级简单的容易，只需要仿照现有的模块编写新的类或函数即可。创建新模块的便利性使得Keras更适合于先进的研究工作。
+ 与Python协作：Keras没有单独的模型配置文件类型（作为对比，caffe有），模型由python代码描述，使其更紧凑和更易debug，并提供了扩展的便利性。

### Keras新特性
+ 泛型模型：简单和强大的新模块，用于支持复杂深度学习模型的搭建。
+ 更优秀的性能：现在，Keras模型的编译时间得到缩短。所有的RNN现在都可以用两种方式实现，以供用户在不同配置任务和配置环境下取得最大性能。现在，基于Theano的RNN也可以被展开，以获得大概25%的加速计算。
+ 测量指标：现在，你可以提供一系列的测量指标来在Keras的任何监测点观察模型性能。
+ 更优的用户体验：我们面向使用者重新编写了代码，使得函数API更简单易记，同时提供更有效的出错信息。
+ 新版本的Keras提供了Lambda层，以实现一些简单的计算任务。

### Keras基本概念
#### 符号计算

Keras的底层库使用Theano或TensorFlow，这两个库也称为Keras的后端。Theano和TensorFlow都是一个“符号主义”的库。

符号主义的计算首先定义各种变量，然后建立一个“计算图”，计算图规定了各个变量之间的计算关系。建立好的计算图需要编译已确定其内部细节，然而，此时的计算图还是一个“空壳子”，里面没有任何实际的数据，只有当你把需要运算的输入放进去后，才能在整个模型中形成数据流，从而形成输出值。

Keras的模型搭建形式就是这种方法，在你搭建Keras模型完毕后，你的模型就是一个空壳子，只有实际生成可调用的函数后（K.function），输入数据，才会形成真正的数据流。

大多数的深度学习框架使用的都是符号计算这一套方法，因为符号计算能够提供关键的计算优化、自动求导等功能。


#### 张量(tensor)

张量可以看作是向量、矩阵的自然推广，我们用张量来表示广泛的数据类型。

规模最小的张量是0阶张量，即标量，也就是一个数。

当我们把一些数有序的排列起来，就形成了1阶张量，也就是一个向量

如果我们继续把一组向量有序的排列起来，就形成了2阶张量，也就是一个矩阵

把矩阵摞起来，就是3阶张量，我们可以称为一个立方体，具有3个颜色通道的彩色图片就是一个这样的立方体

把矩阵摞起来，好吧这次我们真的没有给它起别名了，就叫4阶张量了，不要去试图想像4阶张量是什么样子，它就是个数学上的概念。

张量的阶数有时候也称为维度，或者轴，轴这个词翻译自英文axis。譬如一个矩阵[[1,2],[3,4]]，是一个2阶张量，有两个维度或轴，沿着第0个轴（为了与python的计数方式一致，本文档维度和轴从0算起）你看到的是[1,2]，[3,4]两个向量，沿着第1个轴你看到的是[1,3]，[2,4]两个向量。

要理解“沿着某个轴”是什么意思，不妨试着运行一下下面的代码：
```python
import numpy as np

a = np.array([[1,2],[3,4]])
sum0 = np.sum(a, axis=0)
sum1 = np.sum(a, axis=1)

print(sum0)  # [4 6]
print(sum1)  # [3 7]
```

#### 'th'与'tf'

这是一个无可奈何的问题，在如何表示一组彩色图片的问题上，Theano和TensorFlow发生了分歧，'th'模式，也即Theano模式会把100张RGB三通道的16×32（高为16宽为32）彩色图表示为下面这种形式（100,3,16,32），Caffe采取的也是这种方式。第0个维度是样本维，代表样本的数目，第1个维度是通道维，代表颜色通道数。后面两个就是高和宽了。

而TensorFlow，即'tf'模式的表达形式是（100,16,32,3），即把通道维放在了最后。

Keras默认的数据组织形式在~/.keras/keras.json中规定，可查看该文件的image_dim_ordering一项查看，也可在代码中通过K.image_dim_ordering()函数返回，请在网络的训练和测试中保持维度顺序一致。



#### 泛型模型
Keras的核心数据结构是“模型”，模型是一种组织网络层的方式。Keras中主要的模型是Sequential模型，Sequential是一系列网络层按顺序构成的栈。

在原本的Keras版本中，模型其实有两种:
+ Sequential，称为序贯模型，也就是单输入单输出，一条路通到底，层与层之间只有相邻关系，跨层连接统统没有。这种模型编译速度快，操作上也比较简单
+ Graph，即图模型，这个模型支持多输入多输出，层与层之间想怎么连怎么连，但是编译速度慢。Sequential其实是Graph的一个特殊情况。

在现在这版Keras中，图模型被移除，而增加了了“functional model API”。由于functional model API表达的是“一般的模型”这个概念，我们这里将其译为泛型模型，即只要这个东西接收一个或一些张量作为输入，然后输出的也是一个或一些张量，那不管它是什么鬼，统统都称作“模型”。

#### batch

参数更新的方式方式：

+ Batch gradient descent，批梯度下降
遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习。

+ stochastic gradient descent(SGD)，随机梯度下降
每看一个数据就算一下损失函数，然后求梯度更新参数。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。

+ mini-batch gradient decent，小批的梯度下降
为了克服两种方法的缺点，现在一般采用的是一种折中手段，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。

基本上现在的梯度下降都是基于mini-batch的，所以Keras的模块中经常会出现batch_size，就是指这个。

**转载请注明作者Levine Huang及其出处**

Github博客主页(http://levinehuang.github.io/wiki/)
CSDN博客(http://blog.csdn.net/donhlj)
简书主页(http://www.jianshu.com/users/2bd9b48f6ea8/latest_articles)
**百度搜索levinehuang进入我的博客主页**

### Reference
+ http://keras-cn.readthedocs.io/en/latest/　
