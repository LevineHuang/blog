是什么

定义、原理

有哪些类型，各自的特点是什么

最新进展：替代方案

应用是符合选择

小结

思考问题

从前文[《CNN网络架构演进(一)》](https://mp.weixin.qq.com/s/N1v_VqfWFS8mKjPGo5gtaw)、[《CNN网络架构演进(二)》](https://mp.weixin.qq.com/s/6-eTDDzZyWuzI0ves5qjZw)、[《CNN网络架构演进(三)》](https://mp.weixin.qq.com/s/M3CgfyjeOtOfecqwJvSrAg)中，可以看到，不同卷积神经网络的架构设计中都出现了卷积层(Convolution  Layer)和池化层(Pooling Layer)的身影。在上一节[《深度学习中的卷积方式总结》](https://mp.weixin.qq.com/s/IPcNjhz1_f9A-Lj2W8pYAA)，对深度学习中的卷积方式进行了总结，本节将介绍深度学习中的池化方式，包括池化的定义、原理，池化的不同类型及其各自特点，池化操作的优缺点以及构建神经网络时如何选择适当的池化方式。

#### 池化层介绍

池化层是CNN的重要组成部分，夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合，降低运算复杂程度。

#### 池化层的具体作用

1. 增大感受野，保留主要的特征

2. 增加平移不变性，不变性包括translation(平移)，rotation(旋转)，scale(尺度)

   我们希望目标的些许位置的移动，能得到相同的结果。因为pooling不断地抽象了区域的特征而不关心位置，所以pooling一定程度上增加了平移不变性。

3. 使网络更容易优化，pooling是每个feature map单独做降采样，与基于卷积的降采样相比，不需要参数，更容易优化。在一定程度上防止过拟合，更方便优化


Pooling 的本质是采样。Pooling 对于输入的 Feature Map，选择某种方式对其进行压缩。如下图，表示的就是对 Feature Map ![2 \times 2](https://www.zhihu.com/equation?tex=2+%5Ctimes+2) 邻域内的值，选择最大值输出到下一层，这叫做 Max Pooling。于是一个 ![2N \times 2N](https://www.zhihu.com/equation?tex=2N+%5Ctimes+2N) 的 Feature Map 被压缩到了 ![N \times N](https://www.zhihu.com/equation?tex=N+%5Ctimes+N) 。

![img](https://pic2.zhimg.com/v2-bbcea7d03f0ebd97c7d07c5d133fab5d_b.png)![img](https://pic2.zhimg.com/80/v2-bbcea7d03f0ebd97c7d07c5d133fab5d_hd.png)

Pooling 的意义，主要有两点：

1. 其中一个显而易见，就是减少参数。通过对 Feature Map 降维，有效减少后续层需要的参数
2. 另一个则是 Translation Invariance。它表示对于 Input，当其中像素在邻域发生微小位移时，Pooling Layer 的输出是不变的。这就使网络的鲁棒性增强了，有一定抗扰动的作用



average pooling

max pooling

Lp pooling

mixing pooling

stochastic pooling

spectral pooling

spatial pyramid pooling

multi-scale orderless pooling