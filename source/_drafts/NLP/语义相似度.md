**语义相似度**

---

在NLP领域，语义相似度的计算一直是个难题：搜索场景下query和Doc的语义相似度、feeds场景下Doc和Doc的语义相似度、机器翻译场景下A句子和B句子的语义相似度等等。

以搜索引擎和搜索广告为例，最重要的也最难解决的问题是语义相似度，这里主要体现在两个方面：召回和排序。

在召回时，传统的文本相似性如 BM25，无法有效发现语义类 query-Doc 结果对，如"从北京到上海的机票"与"携程网"的相似性、"快递软件"与"菜鸟裹裹"的相似性。

在排序时，一些细微的语言变化往往带来巨大的语义变化，如"小宝宝生病怎么办"和"狗宝宝生病怎么办"、"深度学习"和"学习深度"。



# **1.DSSM、CDSSM，LSTM-DSSM及相关系列工作**

微软的DSSM及相关系列模型是深度语义模型中比较有影响力的。

## DSSM（Deep Structured Semantic Models）

DSSM [1]（Deep Structured Semantic Models）的原理很简单，通过搜索引擎里 Query 和 Title 的海量的点击曝光日志，用 DNN 把 Query 和 Title 表达为低纬语义向量，并通过 cosine 距离来计算两个语义向量的距离，最终训练出语义相似度模型。该模型既可以用来预测两个句子的语义相似度，又可以获得某句子的低纬语义向量表达。

DSSM首先将query和doc表示成一个高维且稀疏的BOW向量，向量的维度即词典的大小，每一维表示该term在query或doc中出现的频次；

DSSM 从下往上可以分为三层结构：输入层、表示层、匹配层

![img](https://blog-10039692.file.myqcloud.com/1501555296606_1048_1501555297548.png)

### 输入层

输入层做的事情是把句子映射到一个向量空间里并输入到 DNN 中，这里英文和中文的处理方式有很大的不同。

（1）英文

英文的输入层处理方式是通过word hashing。举个例子，假设用 letter-trigams 来切分单词（3 个字母为一组，#表示开始和结束符），boy 这个单词会被切为 #-b-o, b-o-y, o-y-#

![img](https://blog-10039692.file.myqcloud.com/1501555325670_2316_1501555326595.png)

这样做的好处有两个：首先是压缩空间，50 万个词的 one-hot 向量空间可以通过 letter-trigram 压缩为一个 3 万维的向量空间。其次是增强范化能力，三个字母的表达往往能代表英文中的前缀和后缀，而前缀后缀往往具有通用的语义。

这里之所以用 3 个字母的切分粒度，是综合考虑了向量空间和单词冲突：

![img](https://blog-10039692.file.myqcloud.com/1501555347709_7385_1501555348626.png)

以 50 万个单词的词库为例，2 个字母的切分粒度的单词冲突为 1192（冲突的定义：至少有两个单词的 letter-bigram 向量完全相同），而 3 个字母的单词冲突降为 22 效果很好，且转化后的向量空间 3 万维不是很大，综合考虑选择 3 个字母的切分粒度。

（2）中文

中文的输入层处理方式与英文有很大不同，首先中文分词是个让所有 NLP 从业者头疼的事情，即便业界号称能做到 95%左右的分词准确性，但分词结果极为不可控，往往会在分词阶段引入误差。所以这里我们不分词，而是仿照英文的处理方式，对应到中文的最小粒度就是单字了。（曾经有人用偏旁部首切的，感兴趣的朋友可以试试）

由于常用的单字为 1.5 万左右，而常用的双字大约到百万级别了，所以这里出于向量空间的考虑，采用字向量（one-hot）作为输入，向量空间约为 1.5 万维。

### 表示层

DSSM 的表示层采用 BOW（Bag of words）的方式，相当于把字向量的位置信息抛弃了，整个句子里的词都放在一个袋子里了，不分先后顺序。当然这样做会有问题，我们先为 CNN-DSSM 和 LSTM-DSSM 埋下一个伏笔。

紧接着是一个含有多个隐层的 DNN，如下图所示：

![img](https://blog-10039692.file.myqcloud.com/1501555384122_4617_1501555385245.png)

用 Wi 表示第 i 层的权值矩阵，bi 表示第 i 层的 bias 项。则第一隐层向量 l1（300 维），第 i 个隐层向量 li（300 维），输出向量 y（128 维）可以分别表示为：

![img](https://blog-10039692.file.myqcloud.com/1501555503697_9407_1501555504636.png)

用 tanh 作为隐层和输出层的激活函数：

![img](https://blog-10039692.file.myqcloud.com/1501555521224_301_1501555522121.png)

最终输出一个 128 维的低纬语义向量。

### 匹配层

Query 和 Doc 的语义相似性可以用这两个语义向量(128 维) 的 cosine 距离来表示：

![img](https://blog-10039692.file.myqcloud.com/1501555545519_4107_1501555546427.png)

通过softmax 函数可以把Query 与正样本 Doc 的语义相似性转化为一个后验概率：

![img](https://blog-10039692.file.myqcloud.com/1501555590842_9539_1501555591755.png)

其中 r 为 softmax 的平滑因子，D 为 Query 下的正样本，D-为 Query 下的负样本（采取随机负采样），D 为 Query 下的整个样本空间。训练的目标是对同一query下取1个点击doc作为正样本， 随机4个未点击doc作为负样本，让正负样本的区分尽可能大

在训练阶段，通过极大似然估计，我们最小化损失函数：

![img](https://blog-10039692.file.myqcloud.com/1501555602634_219_1501555603542.png)

残差会在表示层的 DNN 中反向传播，最终通过随机梯度下降（SGD）使模型收敛，得到各网络层的参数{Wi,bi}。

### 优缺点

优点：DSSM 用字向量作为输入既可以减少切词的依赖，又可以提高模型的范化能力，因为每个汉字所能表达的语义是可以复用的。另一方面，传统的输入层是用 Embedding 的方式（如 Word2Vec 的词向量）或者主题模型的方式（如 LDA 的主题向量）来直接做词的映射，再把各个词的向量累加或者拼接起来，由于 Word2Vec 和 LDA 都是无监督的训练，这样会给整个模型引入误差，DSSM 采用统一的有监督训练，不需要在中间过程做无监督模型的映射，因此精准度会比较高。

缺点：上文提到 DSSM 采用词袋模型（BOW），因此丧失了语序信息和上下文信息。另一方面，DSSM 采用弱监督、端到端的模型，预测结果不可控。

## CNN-DSSM

针对 DSSM 词袋模型丢失上下文信息的缺点，CLSM[2]（convolutional latent semantic model）应运而生，又叫 CNN-DSSM。CNN-DSSM 与 DSSM 的区别主要在于输入层和表示层。

### 输入层

（1）英文

英文的处理方式，除了上文提到的 letter-trigram，CNN-DSSM 还在输入层增加了word-trigram

![img](https://blog-10039692.file.myqcloud.com/1501555685228_6957_1501555686382.png)

如上图所示，word-trigram其实就是一个包含了上下文信息的滑动窗口。举个例子：把<`s`> online auto body ... <`s`>这句话提取出前三个词<`s`> online auto，之后再分别对这三个词进行letter-trigram映射到一个 3 万维的向量空间里，然后把三个向量 concat 起来，最终映射到一个 9 万维的向量空间里。

（2）中文

英文的处理方式（word-trigram letter-trigram）在中文中并不可取，因为英文中虽然用了 word-ngram 把样本空间拉成了百万级，但是经过 letter-trigram 又把向量空间降到可控级别，只有 3`*`30K（9 万）。而中文如果用 word-trigram，那向量空间就是百万级的了，显然还是字向量（1.5 万维）比较可控。

### 表示层

CNN-DSSM 的表示层由一个卷积神经网络组成，如下图所示：

![img](https://blog-10039692.file.myqcloud.com/1501555818817_3444_1501555820078.png)

（1）卷积层——Convolutional layer

卷积层的作用是提取滑动窗口下的上下文特征。以下图为例，假设输入层是一个 302`*`90000（302 行，9 万列）的矩阵，代表 302 个字向量（query 的和 Doc 的长度一般小于 300，这里少了就补全，多了就截断），每个字向量有 9 万维。而卷积核是一个 3`*`90000 的权值矩阵，卷积核以步长为 1 向下移动，得到的 feature map 是一个 300`*`1 的矩阵，feature map 的计算公式是(输入层维数 302-卷积核大小 3 步长 1)/步长 1=300。而这样的卷积核有 300 个，所以形成了 300 个 300`*`1 的 feature map 矩阵。

![img](https://blog-10039692.file.myqcloud.com/1501555869244_9824_1501555870293.png)

（2）池化层——Max pooling layer

池化层的作用是为句子找到全局的上下文特征。池化层以 Max-over-time pooling 的方式，每个 feature map 都取最大值，得到一个 300 维的向量。Max-over-pooling 可以解决可变长度的句子输入问题（因为不管 Feature Map 中有多少个值，只需要提取其中的最大值）。不过我们在上一步已经做了句子的定长处理（固定句子长度为 302），所以就没有可变长度句子的问题。最终池化层的输出为各个 Feature Map 的最大值，即一个 300`*`1 的向量。这里多提一句，之所以 Max pooling 层要保持固定的输出维度，是因为下一层全链接层要求有固定的输入层数，才能进行训练。

（3）全连接层——Semantic layer

最后通过全连接层把一个 300 维的向量转化为一个 128 维的低维语义向量。全连接层采用 tanh 函数：

![img](https://blog-10039692.file.myqcloud.com/1501555912876_4680_1501555913803.png)

### 匹配层

CNN-DSSM 的匹配层和 DSSM 的一样，这里省略。

### 优缺点

优点：CNN-DSSM 通过卷积层提取了滑动窗口下的上下文信息，又通过池化层提取了全局的上下文信息，上下文信息得到较为有效的保留。

缺点：对于间隔较远的上下文信息，难以有效保留。举个例子，I grew up in France... I speak fluent French，显然 France 和 French 是具有上下文依赖关系的，但是由于 CNN-DSSM 滑动窗口（卷积核）大小的限制，导致无法捕获该上下文信息。

## LSTM-DSSM

针对 CNN-DSSM 无法捕获较远距离上下文特征的缺点，有人提出了用LSTM-DSSM[3]（Long-Short-Term Memory）来解决该问题。

LSTM-DSSM 其实用的是 LSTM 的一个变种——加入了peephole[6]的 LSTM。如下图所示：

![img](https://blog-10039692.file.myqcloud.com/1501556197309_9865_1501556198338.png)

来看一个 LSTM-DSSM 整体的网络结构：

![img](https://blog-10039692.file.myqcloud.com/1501556241446_432_1501556242436.png)

由于DSSM对文本embedding时没有考虑term的顺序信息，又陆续提出了采用Convolution和LSTM对文本embedding，可以保留词序信息。其中，Convolution是实现方式通过对query或doc用固定大小滑动窗口取片段，对每个片段内文本用word-hash+dnn压缩， 然后取max-pooling表示整个query或doc向量。

![img](https://pic4.zhimg.com/80/v2-49187d373877fac7c47a77ecdccfa90a_hd.jpg)

此外， 无论是Convolution还是LSTM对文本embedding, 都涉及到通过词或局部片段的向量生成整个句子的向量，比较简单粗暴的方法是直接取sum、avg或者max等。

红色的部分可以清晰的看到残差传递的方向。

这里列出 DSSM 的 2 个缺点以供参考：

1. DSSM 是端到端的模型，虽然省去了人工特征转化、特征工程和特征组合，但端到端的模型有个问题就是效果不可控。对于一些要保证较高的准确率的场景，用有监督人工标注的 query 分类作为打底，再结合无监督的 word2vec、LDA 等进行语义特征的向量化，显然比较可控（至少 query 分类的准确率可以达到 95%以上）。
2. DSSM 是弱监督模型，因为引擎的点击曝光日志里 Query 和 Title 的语义信息比较弱。举个例子，搜索引擎第一页的信息往往都是 Query 的包含匹配，笔者统计过，完全的语义匹配只有不到 2%。这就意味着几乎所有的标题里都包含用户 Query 里的关键词，而仅用点击和曝光就能作为正负样例的判断？显然不太靠谱，因为大部分的用户进行点击时越靠前的点击的概率越大，而引擎的排序又是由 pCTR、CVR、CPC 等多种因素决定的。从这种非常弱的信号里提取出语义的相似性或者差别，那就需要有海量的训练样本。DSSM 论文中提到，实验的训练样本超过 1 亿。笔者和同事也亲测过，用传统 CTR 预估模型千万级的样本量来训练，模型无法收敛。可是这样海量的训练样本，恐怕只有搜索引擎才有吧？普通的搜索业务 query 有上千万，可资源顶多只有几百万，像论文中说需要挑出点击和曝光置信度比较高且资源热度也比较高的作为训练样本，这样就过滤了 80%的长尾 query 和 Title 结果对，所以也只有搜索引擎才有这样的训练语料了吧。另一方面，超过 1 亿的训练样本作为输入，用深度学习模型做训练，需要大型的 GPU 集群，这个对于很多业务来说也是不具备的条件。

### Siamase LSTM

论文原文：[Siamese Recurrent Architectures for Learning Sentence Similarity](http://www.mit.edu/~jonasm/info/MuellerThyagarajan_AAAI16.pdf)

主要思想是通过一个函数将输入映射到目标空间，在目标空间使用简单的距离（欧式距离等）进行对比相似度。在训练阶段去最小化来自相同类别的一对样本的损失函数值，最大化来自不同类别的一堆样本的损失函数值。

![img](https://img-blog.csdn.net/20171211181640873?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvY29yYWxpbmVfbQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

模型的输入： 
句子1： “He is smart.” 
句子2： “A truly wise man.” 
在输入时，首先需要对句子进分词，以单词为单位通过查表方式转换成对应的预训练好的固定维度的词向量(word embedding)，并按序输入网络。

模型的输出： 
对输入句子的相似性的评价分数。由于模型采用的打分函数为e−xe−x，所以输出的得分区间为 (0, 1]。所以在具体实现的时候，作者将模型的输出进行了相应的变换，将得分的区间变换到了[1, 5]。

句子向量(sentence embedding)： 
本文作者直接将LSTM的最后一个隐藏状态作为句子的embedding。 
或许可以在这一步上进行改近，提示句子向量的性能。

特别说明： 
根据作者的意思，两个LSTM可以是共享权重的，也可以是不共享权重的。在作者提供的代码中，他在进行参数更新时，是将两个LSTM对应参数的梯度进行平均，然后进行更新的。

## 带Attention机制

微软的学者们进一步做了改进，提出利用Attention机制来学习各个词组合成句子向量的权重。以LSTM-DSSM为例，LSTM在每个时间步(term)上输出的隐向量h, 输入给一个attention网络s(h)s(h), 输出权重后softmax归一，然后对每个词的隐向量加权平均生成句子向量。s(h)s(h)的参数和相关性目标一起来训练。这种Attention机制也比较弱，因为不同的query对同一个doc的“关注”点可能是不一样的, 这种方式只能对doc生成唯一的向量。

## local model & distribute model

最近，微软的学者们又提出了一个观点：query与doc的相关程度是由query里的term与doc文本精准的匹配，以及query语义与doc语义匹配程度共同决定。而且，term匹配与term在doc中的位置和紧密度有较大关系。因此，他们用一个local model来表达term匹配程度，distribute model表达语义匹配程度，把这两个子模型放在同一个模型来训练。distribute model类似与DSSM来学习语义匹配关系。Local model的输入是一个nq∗ndnq∗nd的矩阵mm，nqnq是query中term个数，ndnd是doc中term个数，位置m(i,j)=0or1m(i,j)=0or1表示query里的第i个词是否与doc里的第j个词匹配，对这个输入矩阵通过convolution抽取特征并向量化。据其实验结果，这种结合term匹配信息的模型效果要优于DSSM等语义模型。

![img](https://pic3.zhimg.com/80/v2-00ec174653861be85c8e0a8bec297938_hd.jpg)

# **2. Google相关工作**

Google的学者在用convolution对文本向量化是相比CDSSM做了些改进。Convolution的方法参考了Nal Kalchbrenner等对文本用卷积来做分类的方法。

首先，对句子中的每个词做embedding, 然后将词的embedding concat起来组合成一个矩阵，有点类似图像的表达。然后，在这个矩阵上通过不同feature map抽取特征，然后pooling生成一个维度的向量来表达句子。 对Query和Doc的语义向量， 再通过一个bilinear的模型计算其语义相似度：sim(xq,xd)=xq∗M∗xdsim(xq,xd)=xq∗M∗xd。 最终，语义相似度与其它相关排序特征，以及query和doc向量一起作为决定排序的因素，通过pointwise的DNN模型来训练。

![img](https://pic1.zhimg.com/80/v2-3e092adfdd8002c4165336975f8bce5a_hd.jpg)

# **3. IBM Waston实验室相关工作**

`问答系统有很多种类型，其中给定一个Question和候选Answer，从候选Answer中挑选最合适的答案，这个过程与信息检索中的相关性模型非常相似。Waston实验室在InsuranceQA数据集实验了上述类似的模型，并综合CNN和LSTM的优势，提出了几种有意思的混合模型:`

(1) Convolutional-pooling LSTM

用一个Bi-LSTM作为word embedding的方法，然后word embedding concat成矩阵表达句子，用卷积来抽取组合特征作为question和anwser的向量表达，再计算cosin loss.

![img](https://pic3.zhimg.com/80/v2-1010a0b63a462d7df702c27e3e7b80b3_hd.jpg)

（2）Convolution-based LSTM

先对原始文本用卷积捕捉局部的N-gram信息， 然后在这个基础上用Bi-LSTM来学习更大范围的上下文依赖关系。

![img](https://pic2.zhimg.com/80/v2-f5f8facb3edaed008003dda51da3fda5_hd.jpg)

(3) Attentive-LSTM

相比LSTM-DSSM, 在Attention机制上做了些改进，与NMT的Attention机制接近，即：通过Answer中的词向量加权平均生成整个Answer的向量时，每个词的权重是由Question向量和词向量来决定的。Question的表达仍由其所有词向量的avg或sum，max来表示。

![img](https://pic2.zhimg.com/80/v2-2dd029bafcb01cf3a2fe2304fe32b749_hd.jpg)

# **4. 其它相关工作**

上述工作主要集中在如何更好生成Query和Doc向量表达，如何设计两个向量comparision function以计算相似度也有很多种方法。Shuohang Wang总结了6种方法：NN, NTN, EUCCOS, SUB, MULT ，SUBMULT+NN。分别对query和doc向量计算乘、减、欧式距离、cosin、bilinear、concat，以及这几种计算的组合。

![img](https://pic1.zhimg.com/80/v2-6f007a1d4dfdc62fe5c840636fdbe8f2_hd.jpg)

`另外在机器阅读理解也有很多类似工作，本文就不展开描述了。下面介绍下我们的相关工作。`

# **5. 淘宝搜索**

我们对淘宝搜索做了大量的语义改写后，matching不仅局限于term的匹配了，下面分别从数据和模型介绍下我们的工作。

5.1 深度模型通常大量的训练数据，而对商品搜索相关性这个问题，获取大量高质量训练数据并不容易。网页搜索通常直接采用点击数据作为是否相关的label，在商品搜索上不是很有效：用户点击行为与价格、图片、个性化偏好等很多因素相关，仅依赖点击数据对相关性样本有太多噪声； 而采用人工标注数据，准确率相对较高，但受时效性、成本等因素限制较大。最近学术界也逐渐意识到这个问题，提出BM25等无监督模型生成大量样本。我们获取训练数据的方式有：

(1) 对行为数据采样，并用一些类似图像Data Augmentation的手段获取大量(亿级别)准确率相对较低的训练数据，先用这些数据training一个较好的模型；这些方法包括：

a. query下取CTR正常的商品作为正样本， CTR低于平均值较多的商品作为负样本

b. query能召回的类目下随机采样商品作为负样本

c. 对query中的term做一些变换，用变换后的query下点击商品作为原始query的负样本, 例如“红色长袖连衣裙”变换成“蓝色短袖连衣裙”， 而“蓝色短袖连衣裙”下点击商品可以作为“红色长袖连衣裙”下的负样本；

(2) 通过改写模型来为相关性模型生成大量样本，后续可以专门文章介绍这部分；

(3) 采用数量相对少(100w)、准确率高的人工标注数据fine-tuning用上述两种方法pre_training好的模型。



5.2 模型设计主要考虑的几个因素：

(1) 淘宝上Query和商品标题存在大量长尾词，尤其大量数字和英文组合的货号、型号、容量等，分词无法穷尽。仅通过词来对query和标题embedding会损失很多信息，需要考虑字符维度。

(2) 商品除了标题外了，还有图片、类目、属性等信息可以利用。

(3) 工程实现线上计算要轻量，两个向量的compare function要控制计算复杂度。

我们现在采用的模型如下：

![img](https://pic4.zhimg.com/80/v2-b81a767efffd8f68fec77da64887ac85_hd.jpg)

(1) 对Query和标题向量我们采用DNN + Char-LSTM组合的方式：DNN能高效地学到TOP词的embedding, Char-LSTM能捕获到较长尾的字符组合。引入Char-LSTM后模型比较难训练，我们使用query和标题文本语料pretraining LSTM-AutoEncoder, 获得比较好的初始参数；同时TOP词的embedding采用word2vec初始化，模型能更快收敛。

(2) 在商品标题的embedding上增加了一个类目预测的辅助task, 使得不同类目的商品在向量空间内有更好的区分度，对模型效果和收敛速度都有比较好的提升。

(3) online ranking对latency要求比较高，除了工程优化外，模型上也有优化空间。在我们数据上实验发现compare function中全连层的深度和宽度对模型影响比较大。全连层宽一些效果会比较好，但计算量增加会很大；借鉴ResNet全连层设置窄一些，并加深模型，可以保证效果同时较大减少计算量。

我们抽样部分query抓取线上排序结果， 与该模型排序后TOP30人工评测GOOD比例提升1.31%。

5.3 后续计划

商品除了标题和类目，图片也是很重要的信息来源，后续加入图片信息，同时也在尝试用query和商品向量做召回，实现multi-modal检索。

另外，Attention机制也是一个被证明重要的提升点。受限于线上ranking latency的要求，不可能对每个商品标题根据query来计算其"关注"的部分，但可以引入一些self-attention的方法来生成更好的标题向量。

# 参考文献

[1] Shen, Y., He, X., Gao, J., Deng, L., & Mesnil, G. (2014). A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval (pp. 101–110). Presented at the the 23rd ACM International Conference, New York, New York, USA: ACM Press. [http://doi.org/10.1145/2661829.2661935](https://link.zhihu.com/?target=http%3A//doi.org/10.1145/2661829.2661935)

[2] Services, E. U. C. (2014). Learning Deep Structured Semantic Models for Web Search using Clickthrough Data, 1–8.

[3] Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval. (2016). Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval, 1–25.

[4] Zhai, S., Chang, K.-H., Zhang, R., & Zhang, Z. M. (2016). DeepIntent: Learning Attentions for Online Advertising with Recurrent Neural Networks

(pp. 1295–1304). Presented at the the 22nd ACM SIGKDD International Conference, New York, New York, USA: ACM Press. [http://doi.org/10.1145/2939672.2939759](https://link.zhihu.com/?target=http%3A//doi.org/10.1145/2939672.2939759)

[5] Mitra, B., Diaz, F., & Craswell, N. (2016). Learning to Match Using Local and Distributed Representations of Text for Web Search, 1–9.

[6] Improved Representation Learning for Question Answer Matching. (2016). Improved Representation Learning for Question Answer Matching, 1–10.

[7] Feng, M., Xiang, B., Glass, M. R., Wang, L., & Zhou, B. (2015). APPLYING DEEP LEARNING TO ANSWER SELECTION: A STUDY AND AN OPEN TASK , 1–8.

[8] Severyn, A., & Moschitti, A. (2015). Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks (pp. 373–382). Presented at the the 38th International ACM SIGIR Conference, New York, New York, USA: ACM Press.

[9] Kalchbrenner, N., Grefenstette, E., & Blunsom, P. (2014). A Convolutional Neural Network for Modelling Sentences

[10] Wang, S., & Jiang, J. (2017). A COMPARE-AGGREGATE MODEL FOR MATCHING TEXT SEQUENCES, 1–11.

[11] Lin, Z., Feng, M., Santos, dos, C. N., Yu, M., Xiang, B., Zhou, B., & Bengio, Y. (2017). A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING, 1–15.