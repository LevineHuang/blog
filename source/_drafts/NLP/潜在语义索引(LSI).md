# 潜在语义索引

## 潜在语义索引介绍

## 原理

奇异值分解(SVD)

SVD：对于一个m×nm×n的矩阵AA，可以分解为下面三个矩阵：
$$
A_{m \times n} = U_{m \times m}\Sigma_{m \times n} V^T_{n \times n}
$$
有时为了降低矩阵的维度到k，SVD的分解可以近似的写为：
$$
A_{m \times n} \approx U_{m \times k}\Sigma_{k \times k} V^T_{k \times n}
$$
如果把上式用到我们的主题模型，则SVD可以这样解释：我们输入的有m个文本，每个文本有n个词。而$A_{ij}$则对应第i个文本的第j个词的特征值，这里最常用的是基于预处理后的标准化TF-IDF值。k是我们假设的主题数，一般要比文本数少。SVD分解后，$U_{il}$对应第i个文本和第l个主题的相关度。$\Sigma_{lm}$对应第l个主题和第m个词义的相关度。$V_{jm}$对应第j个词和第m个词义的相关度。

通过一次SVD，就可以得到文档和主题的相关度，词和词义的相关度以及词义和主题的相关度。



## 应用场景

1. 获取文本主题词

   得到$U_{mk}$矩阵(第i个文本和第l个主题的相关度)，如何转化为主题词呢？

2. 计算文本相似度

   - 文本与文本相似度

     有三篇文档D1，D2，D3，然后通过LSI得到的文本主题矩阵，可以得到D1，D2，D3之间的相似性，如果新来了一篇文档D4，如何计算D4和D1，D2，D3之间的相似性呢？难道要D1，D2，D3，D4组合起来重新通过LSI得到的文本主题矩阵吗？

   - 关键词与文本相似度

## 应用实践

- 处理流程

  1. 语料的(考虑TF-IDF权重的)向量化表达矩阵

  2. 训练LSI模型，基于LSI降维，得到低维的语料向量化矩阵

  3. 查询文本(考虑TF-IDF权重的)向量化表达，并通过训练好的LSI模型降维

  4. 计算相似度

     ​

- on real corpora, target dimensionality of 200–500 is recommended as a “golden standard” [[1\]](https://radimrehurek.com/gensim/tut2.html#id6).

- 分布式训练

  https://radimrehurek.com/gensim/dist_lsi.html

gensim

```python
import gensim
```



## 小结

词--》词义--》主题=》文本

LSI是最早出现的主题模型了，它的算法原理很简单，一次奇异值分解就可以得到主题模型，同时解决词义的问题，非常漂亮。但是LSI有很多不足，导致它在当前实际的主题模型中已基本不再使用。

　　　　主要的问题有：

　　　　1） SVD计算非常的耗时，尤其是我们的文本处理，词和文本数都是非常大的，对于这样的高维度矩阵做奇异值分解是非常难的。

　　　　2） 主题值的选取对结果的影响非常大，很难选择合适的k值。

　　　　3） LSI得到的不是一个概率模型，缺乏统计基础，结果难以直观的解释。

　　　　对于问题1），主题模型非负矩阵分解（NMF）可以解决矩阵分解的速度问题。对于问题2），这是老大难了，大部分主题模型的主题的个数选取一般都是凭经验的，较新的层次狄利克雷过程（HDP）可以自动选择主题个数。对于问题3），牛人们整出了pLSI(也叫pLSA)和隐含狄利克雷分布(LDA)这类基于概率分布的主题模型来替代基于矩阵分解的主题模型。

　　　　回到LSI本身，对于一些规模较小的问题，如果想快速粗粒度的找出一些主题分布的关系，则LSI是比较好的一个选择，其他时候，如果你需要使用主题模型，推荐使用LDA和HDP。



## 参考

[维基百科-潜在语义索引](https://zh.wikipedia.org/wiki/%E6%BD%9C%E5%9C%A8%E8%AF%AD%E4%B9%89%E7%B4%A2%E5%BC%95)

[文本主题模型之潜在语义索引(LSI)](http://www.cnblogs.com/pinard/p/6805861.html)