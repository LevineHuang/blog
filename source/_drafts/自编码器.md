# 自编码器

$$
h = f(x) \\
r = g(h) = g(f(x))  \\
L = L(x, g(f(x)))
$$
使重构的输出近似的与输入相等，而不是完全相等。

向自编码器添加约束，使它只能近似地复制，并只能复制**与训练数据相似**的输入。

## 自编码器的类型

两种作用

- 将输入复制到输出

- 近似训练数据的概率分布

  ​

### 欠完备自编码器

隐藏层编码维度小于输入维度，即限制h的维度与x小，强制自编码器捕捉训练数据中最显著的特征。

当decoder是线性的且L为均方误差，autoencoder会学习出与PCA相同的生成子空间。

### 过完备自编码器

隐藏层编码维度大于或等于输入维度。

### 正则自编码器

使用的损失函数可以鼓励模型学习到其它特性，而不必限制使用浅层的编码器和解码器以及小的编码器维度来限制模型的容量。

损失函数是什么？

### 稀疏自编码器

损失函数结合编码层的稀疏惩罚项$\Omega(h)$和重构误差：
$$
L(x, g(f(x))) + \Omega(h)
$$
正则项隐含地表达了对函数的偏好。

稀疏惩罚项$\Omega(h)$ ：
$$
\Omega(h) = \lambda \sum_i |h_i|
$$

### 收缩自编码器（CAE，contractive autoencoder）

$$
L(x, g(f(x))) + \Omega(h, x) \\
\Omega(h, x) = \lambda \sum_i ||\nabla _x h_i||^2 = \lambda \sum_i \Vert \frac{\partial f(x)}{\partial x} \Vert ^2_F \\
$$

### 去噪自编码器（DAE，denoising autoencoder）

通过改变损失函数中的重构误差项：
$$
L(x, g(f(\tilde x)))
$$
其中$\tilde x$是被某种噪声损坏的$x$的副本。去噪训练过程强制f， g隐式地学习$p_{data}(x)$的结构



### 变分自编码器

https://zhuanlan.zhihu.com/p/25401928



### 随机生成网络



### 受限玻尔兹曼机



## 自编码器的应用

- 降维或特征学习

  - 提高学习任务的性能，低维空间减少对资源的占用
  - 将语义上相关的样本置于彼此邻近的位置
  - 映射到低维空间所提供的线索有助于泛化

- 信息检索

  - 使搜索变得极为高效

    **语义哈希**：训练encoder生成一个低维且二值的编码。在最终层上使用sigmoid函数，sigmoid单元必须被训练为达到饱和。方法：在sigmoid前加注噪声【？】。

- 图像去噪

  ​




feed_prob

增加noise为什么会提高泛化能力？

dropout为什么会提高泛化能力？

VAE？




## Reference

- [变分自编码器](https://zhuanlan.zhihu.com/p/25401928)