# 非平衡数据处理

## 什么是类不平衡问题

不平衡的类分布（imbalanced class distribution）：指在训练分类器中所使用的训练集的类别分布不均，属于某一类别的观测样本的数量显著少于其它类别。比如说一个二分类问题，1000个训练样本，比较理想的情况是正类、负类样本的数量相差不多；而如果正类样本有995个、负类样本仅5个，就意味着存在类不平衡。

不平衡类别出现在多个领域，包括：

- 欺诈检测、电力盗窃
- 垃圾邮件过滤
- 疾病筛查
- SaaS 客户流失
- 广告点击预测（点击转化率一般都很小）
- 商品推荐（推荐的商品被购买的比例很低）

## **为什么类不平衡是不好的**

### 从模型的训练过程来看

​      从训练模型的角度来说，如果某类的样本数量很少，那么这个类别所提供的“信息”就太少。

​      使用经验风险（模型在训练集上的平均损失）最小化作为模型的学习准则。设损失函数为0-1 loss（这是一种典型的均等代价的损失函数），那么优化目标就等价于错误率最小化（也就是accuracy最大化）。考虑极端情况：1000个训练样本中，正类样本999个，负类样本1个。训练过程中在某次迭代结束后，模型把所有的样本都分为正类，虽然分错了这个负类，但是所带来的损失实在微不足道，accuracy已经是99.9%，于是满足停机条件或者达到最大迭代次数之后自然没必要再优化下去，ok，到此为止，训练结束！于是这个模型……

​      模型没有学习到如何去判别出少数类。

### 从模型的预测过程来看

​      考虑二项Logistic回归模型。输入一个样本 xx ，模型输出的是其属于正类的概率 y^y^ 。当 y^>0.5y^>0.5 时，模型判定该样本属于正类，否则就是属于反类。

​      为什么是0.5呢？可以认为模型是出于最大后验概率决策的角度考虑的，选择了0.5意味着当模型**估计**的样本属于正类的后验概率要大于样本属于负类的后验概率时就将样本判为正类。但实际上，这个后验概率的估计值是否准确呢？

​      从几率（odds）的角度考虑：几率表达的是样本属于正类的可能性与属于负类的可能性的比值。模型对于样本的**预测几率**为 y^1−y^y^1−y^ 。

​      模型在做出决策时，当然希望能够遵循真实样本总体的正负类样本分布：设 θθ 等于正类样本数除以全部样本数，那么样本的**真实几率**为 θ1−θθ1−θ 。当**观测几率大于真实几率**时，也就是 y^>θy^>θ 时，那么就判定这个样本属于正类。

​      虽然我们无法获悉真实样本总体，但之于训练集，存在这样一个**假设：训练集是真实样本总体的无偏采样**。正是因为这个假设，所以认为训练集的**观测几率** θ^1−θ^θ^1−θ^ 就代表了真实几率 θ1−θθ1−θ 。

​      所以，在这个假设下，当一个样本的**预测几率大于观测几率**时，就应该将样本判断为正类。

## **类不平衡问题解决方案**

### 一、解决方案汇总

处理不平衡数据，可以从两方面考虑：

一是改变数据分布，从数据层面使得类别更为平衡；

- 1）过采样：增加少数类样本的数量
- 2）欠采样：减少多数类样本的数量
- 3）综合采样：将过采样和欠采样结合
- 4）Create ensemble balanced sets.

二是改变分类算法，在传统分类算法的基础上对不同类别采取不同的加权方式，使得模型更看重少数类。



- Under-sampling

  Random majority under-sampling with replacement

  Extraction of majority-minority Tomek links [1]
  Under-sampling with Cluster Centroids
  NearMiss-(1 & 2 & 3) [2]
  Condensend Nearest Neighbour [3]
  One-Sided Selection [4]
  Neighboorhood Cleaning Rule [5]
  Edited Nearest Neighbours [6]
  Instance Hardness Threshold [7]
  Repeated Edited Nearest Neighbours [14]
  AllKNN [14]

- Over-sampling

  Random minority over-sampling with replacement
  SMOTE - Synthetic Minority Over-sampling Technique [8]
  bSMOTE(1 & 2) - Borderline SMOTE of types 1 and 2 [9]
  SVM SMOTE - Support Vectors SMOTE [10]
  ADASYN - Adaptive synthetic sampling approach for imbalanced learning [15]

- Over-sampling followed by under-sampling

  SMOTE + Tomek links [12]
  SMOTE + ENN [11]

- Ensemble sampling

  EasyEnsemble [13]
  BalanceCascade [13]

### 二、过采样

#### 2.1 随机过采样

采样算法通过某一种策略改变样本的类别分布，以达到将不平衡分布的样本转化为相对平衡分布的样本的目的，而随机采样是采样算法中最简单也最直观易懂的一种方法。

随机过抽样是增加少数类样本数量，可以事先设置多数类与少数类最终的数量比例，在保留多数类样本不变的情况下，根据比例随机复制少数类样本，在使用的过程中为了保证所有的少数类样本信息都会被包含，可以先完全复制一份全量的少数类样本，再随机复制少数样本使得满足数量比例，具体步骤如下：

1.首先在少数类Smin集合中随机选中一些少数类样本
2.然后通过复制所选样本生成样本集合E
3.将它们添加到Smin中来扩大原始数据集从而得到新的少数类集合Smin−new

Smin中的总样本数增加了 |E|个新样本，且Smin−new 的类分布均衡度进行了相应的调整，如此操作可以改变类分布平衡度从而达到所需水平。

重复样本过多，容易造成分类器的过拟合

#### 2.2 SMOTE算法(Synthetic Minority Oversampling Technique)

在合成抽样技术方面，Chawla NY等人提出的SMOTE过抽样技术是基于随机过采样算法的一种改进方案，由于随机过采样简单复制样本的策略来增加少数类样本，这样容易产生模型过拟合的问题，即使模型学习到的信息过于特别（Specific）而不够泛化(General)。

SMOTE的主要思想是利用特征空间中现存少数类样本之间的相似性来建立人工数据，特别是，对于子集Smin⊂⊂S，对于每一个样本xi⊂Smin使用K-近邻法，其中K-近邻被定义为考虑SminSmin中的K个元素本身与xixi的欧氏距离在n维特征空间X中表现为最小幅度值的样本。由于不是简单地复制少数类样本，因此可以在一定程度上避免分类器的过度拟合，实践证明此方法可以提高分类器的性能。但是由于对每个少数类样本都生成新样本，因此容易发生生成样本重叠（overlapping）的问题。算法流程如下：

- 1）对于少数类中的每一个样本(xi)(xi)，以欧氏距离为标准计算它到少数类样本集SminSmin中所有样本的距离，得到K近邻；
- 2）根据样本不平衡比例设置一个采样比例以确定采样倍率N，对于每一个少数类样本xixi，从其K近邻中随机选择若干个样本，
  假设选择的近邻为x~x~；
- 3）对于每一个随机选出的近邻x~x~，分别与原样本按照如下的公式构建新的样本:xnew=x+rand(0,1)×(x~−x)xnew=x+rand(0,1)×(x~−x)

[![屏幕快照 2017-04-19 上午10.01.40](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.01.40.png)](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.01.40.png)

#### 2.3 Borderline-SMOTE算法

原始的SMOTE算法对所有的少数类样本都是一视同仁的，但实际建模过程中发现那些处于边界位置的样本更容易被错分，因此利用边界位置的样本信息产生新样本可以给模型带来更大的提升。Borderline-SMOTE便是将原始SMOTE算法和边界信息算法结合的算法。算法流程如下：

1.首先，对于每个xi⊂Smin确定一系列K-近邻样本集，称该数据集为Si−kNN，且Si−kNN⊂S；
2.然后，对每个样本xi，判断出最近邻样本集中属于多数类样本的个数，即：|Si−kNN∩Smaj|；
3.最后，选择满足下面不等式的xi:k/2<|Si−kNN∩Smaj|<k,将其加入危险集DANGER，

对危险集中的每一个样本点（最容易被错分的样本），采用普通的SMOTE算法生成新的少数类样本。
[![屏幕快照 2017-04-19 上午10.02.36](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.02.36.png)](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.02.36.png)

### 三、欠采样

#### 3.1 随机欠采样

减少多数类样本数量最简单的方法便是随机剔除多数类样本，可以事先设置多数类与少数类最终的数量比例，在保留少数类样本不变的情况下，根据比例随机选择多数类样本。

- 1）首先我们从Smaj中随机选取一些多数类样本E
- 2）将这些样本从Smaj中移除，就有|Smaj−new|=|Smaj−|ESmaj−new|=|Smaj−|E|

优点在于操作简单，只依赖于样本分布，不依赖任何距离信息，属于非启发式方法；缺点在于会丢失一部分多数类样本的信息，无法充分利用已有信息。

#### 3.2 Tomek Links方法

定义：Tomek links被定义为**相反类**最近邻样本之间的一对连接。

符号约定：给定一个样本对(xi,xj)，其中xi∈ Smaj，xj ∈ Smin，记d(xi,xj)是样本xi和xj之间的距离

公式表示：如果不存在任何样本xk，使得d(xi,xk) <d(xi,xj) ，那么样本对(xi,xj)被称为Tomek Links

使用这种方法，如果两个样本来自Tomek Links，那么他们中的一个样本要么是噪声要么它们都在两类的边界上。所以Tomek Links一般有两种用途：在欠采样中：将Tomek Links中属于是多数类的样本剔除；在数据清洗中，将Tomek Links中的两个样本都剔除。
[![屏幕快照 2017-04-19 上午10.07.11](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.07.11.png)](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.07.11.png)

#### 3.3 NearMiss方法

NearMiss方法是利用距离远近剔除多数类样本的一类方法，实际操作中也是借助KNN，总结起来有以下几类：

- 1）NearMiss-1：在多数类样本中选择与最近的三个少数类样本的平均距离最小的样本
- 2）NearMiss-2：在多数类样本中选择与最远的3个少数类样本的平均距离最小的样本
- 3）NearMiss-3：对于每个少数类样本，选择离它最近的给定数量的多数类样本

NearMiss-1和NearMiss-2方法的描述仅有一字之差，但其含义是完全不同的：NearMiss-1考虑的是与最近的3个少数类样本的平均距离，是局部的；NearMiss-2考虑的是与最远的3个少数类样本的平均距离，是全局的。

NearMiss-1方法得到的多数类样本分布也是”不均衡“的，它倾向于在比较集中的少数类附近找到更多的多数类样本，而在孤立的（或者说是离群的）少数类附近找到更少的多数类样本，原因是NearMiss-1方法考虑的局部性质和平均距离。

NearMiss-3方法则会使得每一个少数类样本附近都有足够多的多数类样本，显然这会使得模型的精确度高、召回率低。

实验结果表明得到NearMiss-2的不均衡分类性能最优。

### 四、综合采样

目前为止我们使用的重采样方法几乎都是只针对某一类样本：对多数类样本欠采样，对少数类样本过采样。也有人提出将欠采样和过采样综合的方法，解决样本类别分布不平衡和过拟合问题，本部分介绍其中的SMOTE+Tomek Links和SMOTE+ENN。

#### 4.1 SMOTE+Tomek Links

SMOTE+Tomek Links方法的算法流程非常简单：
1.利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T
2.剔除T中的Tomek Links对

普通的SMOTE方法生成的少数类样本是通过线性插值得到的，在平衡类别分布的同时也扩张了少数类的样本空间，产生的问题是可能原本属于多数类样本的空间被少数类“入侵”，容易造成模型的过拟合。

Tomek Links对寻找的是那种噪声点或者边界点，可以很好地解决“入侵”的问题，下图红色加号为SMOTE产生的少数类样本，可以看到，红色样本“入侵”到原本属于多数类样本的空间，这种噪声数据问题可以通过Tomek Links很好地解决。

[![屏幕快照 2017-04-19 上午10.08.56](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.08.56.png)](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.08.56.png)

由于第一步SMOTE方法已经很好地平衡了类别分布，因此在使用Tomek Links对的时候考虑剔除所有的Tomek Links对。

#### 4.2 SMOTE+KNN

SMOTE+KNN方法和SMOTE+Tomek Links方法的想法和过程都是很类似的：

- 1）利用SMOTE方法生成新的少数类样本，得到扩充后的数据集T
- 2）对T中的每一个样本使用KNN（一般K取3）方法预测，若预测结果与实际类别标签不符，则剔除该样本。



### 五、Informed Understanding

Informed欠抽样算法可以解决传统随机欠采样造成的数据信息丢失问题，且表现出较好的不均衡数据分类性能。其中有一些集成（ensemble）的想法，主要有两种方法，分别是EasyEnsemble算法和BalanceCascade算法。

#### 5.1 EasyEnsemble算法

它把数据划分为两部分，分别是多数类样本和少数类样本，对于多数类样本Smaj，通过n次有放回抽样生成n份子集，少数类样本Smin分别和这n份样本合并训练AdaBoost分类器，这样可以得到n个模型，最终的模型采用加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率小的弱分类器的权值，使其在表决中起较小的作用。这里假设多数类样本为N，少数类样本为P，算法流程如下：
[![屏幕快照 2017-04-19 上午10.07.45](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.07.45.png)](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.07.45.png)

EasyEnsemble的想法是多次随机欠抽样，尽可能全面地涵盖所有信息，算法特点是利用boosting减小偏差（Adaboost）、bagging减小方差（集成分类器）。实际应用的时候也可以尝试选用不同的分类器来提高分类的效果。

[![屏幕快照 2017-04-19 上午10.08.08](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.08.08.png)](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.08.08.png)

#### 5.2 BalanceCascade算法

EasyEnsemble算法训练的子过程是独立的，BalanceCascade则是一种级联算法，这种级联的思想在图像识别中用途非常广泛。算法流程如下：
[![屏幕快照 2017-04-19 上午10.08.28](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.08.28.png)](http://omu7tit09.bkt.clouddn.com/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202017-04-19%20%E4%B8%8A%E5%8D%8810.08.28.png)

BalanceCascade算法得到的是一个级联分类器，将若干个强分类器由简单到复杂排列，只有和少数类样本特征比较接近的才有可能输入到后面的分类器，比如边界点，因此能更充分地利用多数类样本的信息，一定程度上解决随机欠采样的信息丢失问题。



## Reference

- https://www.cnblogs.com/Determined22/p/5772538.html
- https://pypi.python.org/pypi/imbalanced-learn
- https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650724464&idx=1&sn=1f34358862bacfb4c7ea17c864d8c44d&chksm=871b1c0eb06c95180e717d8316b0380602f638a764530b4b9e35ac812c7c33799d3357d46f00&scene=0&key=0f5e635eeb6bf20a076ad60d7f11c6ef5c5c1c8f02873bc8b458381b629a1e2ae76174d0d4ba34331c71d095e3b3b92aa7fff5e1e11badeaf6c87ff90fd264f3dc6b1eb074eaccb2ac46e8f2d440cefd&ascene=0&uin=MTU1NTY3MTA0Mg%3D%3D&devicetype=iMac+MacBookPro12%2C1+OSX+OSX+10.11.6+build(15G1217)&version=12010310&nettype=WIFI&fontScale=100&pass_ticket=csWk%2BJXfpl7rA8r527fLqF%2BF3EZEeBKpFRjI%2BWMXoPf2PEtPt%2FLMrscLX4GBl7gg)