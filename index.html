<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Welcome to Levine&#39;s Wikis</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Welcome to Levine's Wikis">
<meta property="og:url" content="http://levinehuang.github.io/index.html">
<meta property="og:site_name" content="Welcome to Levine's Wikis">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Welcome to Levine's Wikis">
  
    <link rel="alternate" href="/atom.xml" title="Welcome to Levine&#39;s Wikis" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/wiki/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/wiki/" id="logo">Welcome to Levine&#39;s Wikis</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/wiki/">Home</a>
        
          <a class="main-nav-link" href="/wiki/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://levinehuang.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-A05-MI/02-TensorFlow/multilayer_perceptron" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/08/08/A05-MI/02-TensorFlow/multilayer_perceptron/" class="article-date">
  <time datetime="2017-08-08T01:26:33.729Z" itemprop="datePublished">2017-08-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="string">'''</span></div><div class="line">A Multilayer Perceptron implementation example using TensorFlow library.</div><div class="line">This example is using the MNIST database of handwritten digits</div><div class="line">(http://yann.lecun.com/exdb/mnist/)</div><div class="line"></div><div class="line">Author: Aymeric Damien</div><div class="line">Project: https://github.com/aymericdamien/TensorFlow-Examples/</div><div class="line">'''</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Import MINST data</span></div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line">mnist = input_data.read_data_sets(<span class="string">"../../data/"</span>, one_hot=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div></pre></td></tr></table></figure>
<pre><code>Extracting ../../data/train-images-idx3-ubyte.gz
Extracting ../../data/train-labels-idx1-ubyte.gz
Extracting ../../data/t10k-images-idx3-ubyte.gz
Extracting ../../data/t10k-labels-idx1-ubyte.gz
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Parameters</span></div><div class="line">learning_rate = <span class="number">0.001</span></div><div class="line">training_epochs = <span class="number">15</span></div><div class="line">batch_size = <span class="number">100</span></div><div class="line">display_step = <span class="number">1</span></div><div class="line"></div><div class="line"><span class="comment"># Network Parameters</span></div><div class="line">n_hidden_1 = <span class="number">256</span> <span class="comment"># 1st layer number of features</span></div><div class="line">n_hidden_2 = <span class="number">256</span> <span class="comment"># 2nd layer number of features</span></div><div class="line">n_input = <span class="number">784</span> <span class="comment"># MNIST data input (img shape: 28*28)</span></div><div class="line">n_classes = <span class="number">10</span> <span class="comment"># MNIST total classes (0-9 digits)</span></div><div class="line"></div><div class="line"><span class="comment"># tf Graph input</span></div><div class="line">x = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_input])</div><div class="line">y = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>, n_classes])</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Create model</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">multilayer_perceptron</span><span class="params">(x, weights, biases)</span>:</span></div><div class="line">    <span class="comment"># Hidden layer with RELU activation</span></div><div class="line">    layer_1 = tf.add(tf.matmul(x, weights[<span class="string">'h1'</span>]), biases[<span class="string">'b1'</span>])</div><div class="line">    layer_1 = tf.nn.relu(layer_1)</div><div class="line">    <span class="comment"># Hidden layer with RELU activation</span></div><div class="line">    layer_2 = tf.add(tf.matmul(layer_1, weights[<span class="string">'h2'</span>]), biases[<span class="string">'b2'</span>])</div><div class="line">    layer_2 = tf.nn.relu(layer_2)</div><div class="line">    <span class="comment"># Output layer with linear activation</span></div><div class="line">    out_layer = tf.matmul(layer_2, weights[<span class="string">'out'</span>]) + biases[<span class="string">'out'</span>]</div><div class="line">    <span class="keyword">return</span> out_layer</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Store layers weight &amp; bias</span></div><div class="line">weights = &#123;</div><div class="line">    <span class="string">'h1'</span>: tf.Variable(tf.random_normal([n_input, n_hidden_1])),</div><div class="line">    <span class="string">'h2'</span>: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),</div><div class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_hidden_2, n_classes]))</div><div class="line">&#125;</div><div class="line">biases = &#123;</div><div class="line">    <span class="string">'b1'</span>: tf.Variable(tf.random_normal([n_hidden_1])),</div><div class="line">    <span class="string">'b2'</span>: tf.Variable(tf.random_normal([n_hidden_2])),</div><div class="line">    <span class="string">'out'</span>: tf.Variable(tf.random_normal([n_classes]))</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment"># Construct model</span></div><div class="line">pred = multilayer_perceptron(x, weights, biases)</div><div class="line"></div><div class="line"><span class="comment"># Define loss and optimizer</span></div><div class="line">cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))</div><div class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</div><div class="line"></div><div class="line"><span class="comment"># Initializing the variables</span></div><div class="line">init = tf.global_variables_initializer()</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># tf.equal(A, B)是对比这两个矩阵或者向量的相等的元素，如果是相等的那就返回True，反正返回False，返回的值的矩阵维度和A是一样的</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">A = [[<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</div><div class="line">B = [[<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>]]</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    print(sess.run(tf.equal(A, B)))</div></pre></td></tr></table></figure>
<pre><code>[[ True  True  True False False]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># tf.argmax(vector, 1)：返回的是vector中的最大值的索引号，如果vector是一个向量，那就返回一个值，</span></div><div class="line"><span class="comment"># 如果是一个矩阵，那就返回一个向量，这个向量的每一个维度都是相对应矩阵行的最大值元素的索引号。</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf  </div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </div><div class="line">A = [[<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]  </div><div class="line">B = [[<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>], [<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>]]  </div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:  </div><div class="line">    print(sess.run(tf.argmax(A, <span class="number">1</span>)))  </div><div class="line">    print(sess.run(tf.argmax(B, <span class="number">1</span>)))</div><div class="line"></div><div class="line"><span class="comment"># tf.cast：用于改变某个张量的数据类型</span></div><div class="line"></div><div class="line">A = tf.convert_to_tensor(np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>], [<span class="number">3</span>,<span class="number">4</span>,<span class="number">8</span>,<span class="number">5</span>]]))  </div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:  </div><div class="line">    print(A.dtype)</div><div class="line">    b = tf.cast(A, tf.float32)  </div><div class="line">    print(b.dtype)</div></pre></td></tr></table></figure>
<pre><code>[4]
[2 1]
&lt;dtype: &apos;int64&apos;&gt;
&lt;dtype: &apos;float32&apos;&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Launch the graph</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(init)</div><div class="line"></div><div class="line">    <span class="comment"># Training cycle</span></div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</div><div class="line">        avg_cost = <span class="number">0.</span></div><div class="line">        total_batch = int(mnist.train.num_examples/batch_size)</div><div class="line">        <span class="comment"># Loop over all batches</span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(total_batch):</div><div class="line">            batch_x, batch_y = mnist.train.next_batch(batch_size)</div><div class="line">            <span class="comment"># Run optimization op (backprop) and cost op (to get loss value)</span></div><div class="line">            _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_x,</div><div class="line">                                                          y: batch_y&#125;)</div><div class="line">            <span class="comment"># Compute average loss</span></div><div class="line">            avg_cost += c / total_batch</div><div class="line">        <span class="comment"># Display logs per epoch step</span></div><div class="line">        <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch+<span class="number">1</span>), <span class="string">"cost="</span>, \</div><div class="line">                <span class="string">"&#123;:.9f&#125;"</span>.format(avg_cost))</div><div class="line">    print(<span class="string">"Optimization Finished!"</span>)</div><div class="line"></div><div class="line">    <span class="comment"># Test model</span></div><div class="line">    correct_prediction = tf.equal(tf.argmax(pred, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</div><div class="line">    <span class="comment"># Calculate accuracy</span></div><div class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</div><div class="line">    print(<span class="string">"Accuracy:"</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;))</div></pre></td></tr></table></figure>
<pre><code>Epoch: 0001 cost= 201.333574999
Epoch: 0002 cost= 46.010647814
Epoch: 0003 cost= 28.918377875
Epoch: 0004 cost= 20.020559532
Epoch: 0005 cost= 14.741128482
Epoch: 0006 cost= 11.113235143
Epoch: 0007 cost= 8.444069761
Epoch: 0008 cost= 6.217341204
Epoch: 0009 cost= 4.772409531
Epoch: 0010 cost= 3.578625235
Epoch: 0011 cost= 2.738378037
Epoch: 0012 cost= 1.975170798
Epoch: 0013 cost= 1.481307366
Epoch: 0014 cost= 1.183054283
Epoch: 0015 cost= 0.933368129
Optimization Finished!
Accuracy: 0.9483
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/08/08/A05-MI/02-TensorFlow/multilayer_perceptron/" data-id="cj8ijf1uj001oyoo0t47emap8" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-A05-MI/03-SparkMLlib/SparkMLlib之01-Spark机器学习库介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/05/07/A05-MI/03-SparkMLlib/SparkMLlib之01-Spark机器学习库介绍/" class="article-date">
  <time datetime="2017-05-07T11:59:27.120Z" itemprop="datePublished">2017-05-07</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h4 id="Spark机器学习库简介"><a href="#Spark机器学习库简介" class="headerlink" title="Spark机器学习库简介"></a>Spark机器学习库简介</h4><p>MLlib是Spark的机器学习库。旨在简化机器学习的工程实践工作，并方便扩展到更大规模。MLlib由一些通用的学习算法和工具组成，包括分类、回归、聚类、协同过滤、降维等，同时还包括底层的优化原语和高层的管道API。</p>
<p>它提供如下工具：</p>
<ol>
<li>机器学习算法：常规机器学习算法包括分类、回归、聚类和协同过滤。</li>
<li>特征工程：特征提取、特征转换、特征选择以及降维。</li>
<li>管道：构造、评估和调整的管道的工具。</li>
<li>存储：保存和加载算法、模型及管道。</li>
<li>实用工具：线性代数，统计，数据处理等。</li>
</ol>
<h4 id="Spark机器学习库MLlib和ML"><a href="#Spark机器学习库MLlib和ML" class="headerlink" title="Spark机器学习库MLlib和ML"></a>Spark机器学习库MLlib和ML</h4><p>Spark机器学习库从 1.2 版本以后被分为两个包，分别是：</p>
<ul>
<li><p>spark.mllib 包含基于RDD的原始算法API。<br>Spark MLlib 历史比较长，1.0 以前的版本中已经包含了，提供的算法实现都是基于原始的 RDD，从学习角度上来讲，其实比较容易上手。如果您已经有机器学习方面的经验，那么您只需要熟悉下 MLlib 的 API 就可以开始数据分析工作了。想要基于这个包提供的工具构建完整并且复杂的机器学习流水线是比较困难的。</p>
</li>
<li><p>spark.ml 则提供了基于DataFrames 高层次的API，可以用来构建机器学习管道。<br>Spark ML Pipeline 从 Spark1.2 版本开始，成为可用并且较为稳定的新的机器学习库。ML Pipeline 弥补了原始 MLlib 库的不足，向用户提供了一个基于 DataFrame 的机器学习工作流式 API 套件，使用 ML Pipeline API，我们可以很方便的把数据处理，特征转换，正则化，以及多个机器学习算法联合起来，构建一个单一完整的机器学习流水线。显然，这种新的方式给我们提供了更灵活的方法，而且这也更符合机器学习过程的特点。</p>
</li>
</ul>
<p>从官方文档来看，Spark ML Pipeline 虽然是被推荐的机器学习方式，但是并不会在短期内替代原始的 MLlib 库，因为 MLlib 已经包含了丰富稳定的算法实现，并且部分 ML Pipeline 实现基于 MLlib。</p>
<p>在Spark2.0中，spark.mllib包中的RDD接口已进入维护模式。现在主要的机器学习接口为spark.ml包中的基于数据框接口。</p>
<p>这一转变包含哪些信息？</p>
<ol>
<li>MLlib将继续在spark.mllib中支持基于RDD的接口。</li>
<li>MLlib不会向基于RDD的接口中继续添加新的特征。</li>
<li>在Spark2.0以后的版本中，将继续向基于数据框的接口添加新特征以缩小与基于RDD接口的差异。</li>
<li>当两种接口之间达到特征相同时（初步估计为Spark2.2），基于RDD的接口将被废弃。</li>
<li>基于RDD的接口将在Spark3.0中被移除。</li>
</ol>
<p>为什么MLlib转向DataFrames接口？</p>
<ol>
<li>数据框可以提供比RDD更容易掌握使用的接口。数据框的主要优点包括Spark数据源来源、结构化查询语言的数据框查询、各编程语言之间统一的接口。</li>
<li>基于数据框的MLlib接口为多种机器学习算法与编程语言提供统一的接口。</li>
<li>数据框有助于实现机器学习管道，特别是特征转换。</li>
</ol>
<h4 id="Spark机器学习库的功能"><a href="#Spark机器学习库的功能" class="headerlink" title="Spark机器学习库的功能"></a>Spark机器学习库的功能</h4><p>下面的列表列出了两个包的主要功能。</p>
<h5 id="spark-mllib-数据类型，算法以及工具"><a href="#spark-mllib-数据类型，算法以及工具" class="headerlink" title="spark.mllib: 数据类型，算法以及工具"></a><a href="http://spark.apache.org/docs/latest/mllib-guide.html" target="_blank" rel="external">spark.mllib: 数据类型，算法以及工具</a></h5><p>Data types（数据类型）<br>Basic statistics（基础统计）<br>summary statistics（摘要统计）<br>correlations（相关性）<br>stratified sampling（分层抽样）<br>hypothesis testing（假设检验）<br>streaming significance testing<br>random data generation（随机数据生成）<br>Classification and regression（分类和回归）<br>linear models (SVMs, logistic regression, linear regression)（线性模型（SVM，逻辑回归，线性回归））<br>naive Bayes（朴素贝叶斯）<br>decision trees（决策树）<br>ensembles of trees (Random Forests and Gradient-Boosted Trees)（树套装（随机森林和梯度提升决策树））<br>isotonic regression（保序回归）<br>Collaborative filtering（协同过滤）<br>alternating least squares (ALS)（交替最小二乘（ALS））<br>Clustering（聚类）<br>k-means（K-均值）<br>Gaussian mixture（高斯混合）<br>power iteration clustering (PIC)（幂迭代聚类（PIC））<br>latent Dirichlet allocation (LDA)（隐含狄利克雷分配）<br>bisecting k-means（平分K-均值）<br>streaming k-means（流式K-均值）<br>Dimensionality reduction（降维）<br>singular value decomposition (SVD)（奇异值分解（SVD））<br>principal component analysis (PCA)（主成分分析（PCA））<br>Feature extraction and transformation（特征抽取和转换）<br>Frequent pattern mining（频繁模式挖掘）<br>FP-growth（FP-增长）<br>association rules（关联规则）<br>PrefixSpan（PrefixSpan）<br>Evaluation metrics（评价指标）<br>PMML model export（PMML模型导出）<br>Optimization (developer)（优化（开发者））<br>stochastic gradient descent（随机梯度下降）<br>limited-memory BFGS (L-BFGS)（有限的记忆BFGS（L-BFGS））</p>
<h5 id="spark-ml-机器学习管道高级API"><a href="#spark-ml-机器学习管道高级API" class="headerlink" title="spark.ml: 机器学习管道高级API"></a><a href="http://spark.apache.org/docs/latest/ml-pipeline.html" target="_blank" rel="external">spark.ml: 机器学习管道高级API</a></h5><p>Overview: estimators, transformers and pipelines（概览：评估器，转换器和管道）<br>Extracting, transforming and selecting features（抽取，转换和选取特征）<br>Classification and regression（分类和回归）<br>Clustering（聚类）<br>Advanced topics（高级主题）</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/05/07/A05-MI/03-SparkMLlib/SparkMLlib之01-Spark机器学习库介绍/" data-id="cj8ijf1uc001kyoo0zllzr8w3" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-A01-BigData/02-Spark/SparkSQL" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/03/22/A01-BigData/02-Spark/SparkSQL/" class="article-date">
  <time datetime="2017-03-21T23:54:47.106Z" itemprop="datePublished">2017-03-22</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>sql sqlText sqlParser   UnresolvedLogicalPlan<br>    Analyzer ResolvedLogicalPlan<br>    Optimizer OptimizedLogicalPlan<br>    SparkPlanner PhysicalPlan<br>              prepareForExcution<br>              execute</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/03/22/A01-BigData/02-Spark/SparkSQL/" data-id="cj8ijf1u4001eyoo0r3ab7fwb" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-A05-MI/10-DL理论/DL之05-RNN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/03/09/A05-MI/10-DL理论/DL之05-RNN/" class="article-date">
  <time datetime="2017-03-09T00:58:06.902Z" itemprop="datePublished">2017-03-09</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="Language-Model"><a href="#Language-Model" class="headerlink" title="Language Model"></a>Language Model</h3><p>to predict the probability of observing the sentence (in a given dataset) as:</p>
<p>\begin{aligned}  P(w_1,…,w<em>m) = \prod</em>{i=1}^{m} P(w_i \mid w<em>1,…, w</em>{i-1})  \end{aligned}</p>
<p>计算一个句子出现概率有什么作用s</p>
<ul>
<li>翻译</li>
<li>语音识别</li>
<li>generative model - Because we can predict the probability of a word given the preceding words, we are able to generate new text</li>
</ul>
<p>缺点：当前置单词很多时，RNN遇到困难，引出LSTM。</p>
<h3 id="训练数据及其预处理"><a href="#训练数据及其预处理" class="headerlink" title="训练数据及其预处理"></a>训练数据及其预处理</h3><p>15000条评论数据</p>
<h4 id="分词-tokenize-text"><a href="#分词-tokenize-text" class="headerlink" title="分词  tokenize text"></a>分词  tokenize text</h4><p>借助NLTK中的word_tokenize和sent_tokenize方法</p>
<h4 id="去除低频词"><a href="#去除低频词" class="headerlink" title="去除低频词"></a>去除低频词</h4><p>原因：</p>
<ul>
<li>太多的词，模型难以训练</li>
<li>低频词没有足够的上下文样本数据</li>
</ul>
<p>低频词的处理方式：<br>按词频选取前N个（如8000），其它即为低频词，用“UNKNOWN_TOKEN”代替。生产新文本后，随机取词汇库外的一个词替换“UNKNOWN_TOKEN”，或者不断生产新文本，直到生成文本中不包含“UNKNOWN_TOKEN”为止。</p>
<h3 id="梯度消失问题-THE-VANISHING-GRADIENT-PROBLEM"><a href="#梯度消失问题-THE-VANISHING-GRADIENT-PROBLEM" class="headerlink" title="梯度消失问题 THE VANISHING GRADIENT PROBLEM"></a>梯度消失问题 THE VANISHING GRADIENT PROBLEM</h3><ul>
<li>W矩阵初始化</li>
<li>正则化 用ReLU代替tanh或 sigmoid激活函数】</li>
<li>采用LSTM或GRU(Gated Recurrent Unit, LSTM的简化版本)结构</li>
</ul>
<h2 id="Part4"><a href="#Part4" class="headerlink" title="Part4"></a>Part4</h2><h3 id="LSTM网络"><a href="#LSTM网络" class="headerlink" title="LSTM网络"></a>LSTM网络</h3><p>通过gating mechanism机制解决RNNs的梯度消失问题。<br>plain RNNs可以看做是LSTMs的特殊形式(input gate 取1，forget gate 取0，output gate 取1)</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/03/09/A05-MI/10-DL理论/DL之05-RNN/" data-id="cj8ijf1ut001syoo05q3sgkdd" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-A05-MI/10-DL理论/LSTM" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/03/01/A05-MI/10-DL理论/LSTM/" class="article-date">
  <time datetime="2017-03-01T03:45:57.393Z" itemprop="datePublished">2017-03-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>window选择:</p>
<ul>
<li>固定时间跨度</li>
<li>固定数据长度，数据个数</li>
<li>window内的数据个数 + 数据方差</li>
</ul>
<h3 id="LSTM的应用"><a href="#LSTM的应用" class="headerlink" title="LSTM的应用"></a>LSTM的应用</h3><h4 id="序列分类"><a href="#序列分类" class="headerlink" title="序列分类"></a>序列分类</h4><ul>
<li><a href="http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/" target="_blank" rel="external">http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/</a><br>预测随空间或时间变化的序列的类别</li>
</ul>
<p>电影评论情感分类</p>
<p>word embedding</p>
<p>One-hot Representation<br>把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。</p>
<p>缺点：<br>维数灾难[Bengio 2003]<br>“词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系，哪怕是话筒和麦克这样的同义词也不能幸免于难。</p>
<p>Distributed representation<br> 最大的贡献就是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用 cos 夹角来衡量。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/03/01/A05-MI/10-DL理论/LSTM/" data-id="cj8ijf1uv001tyoo0prw7ucd6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-abc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/02/28/abc/" class="article-date">
  <time datetime="2017-02-28T04:17:00.000Z" itemprop="datePublished">2017-02-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/wiki/categories/MachineLearning/">MachineLearning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/wiki/2017/02/28/abc/">ML之01-核密度估计Kernel Density Estimation(KDE)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="ML之01-核密度估计Kernel-Density-Estimation-KDE"><a href="#ML之01-核密度估计Kernel-Density-Estimation-KDE" class="headerlink" title="ML之01-核密度估计Kernel Density Estimation(KDE)"></a>ML之01-核密度估计Kernel Density Estimation(KDE)</h2><hr>
<p>由给定样本集合求解随机变量的分布密度函数问题是概率统计学的基本问题之一。解决这一问题的方法包括参数估计和非参数估计。</p>
<h3 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h3><ul>
<li>参数回归分析<br>需要假定数据分布符合某种特定的性态，如线性、可化线性或指数性态等，然后在目标函数族中寻找特定的解，即确定回归模型中的未知参数。</li>
<li>参数判别分析<br>需要假定作为判别依据的、随机取值的数据样本在各个可能的类别中都服从特定的分布。</li>
</ul>
<p>经验和理论说明，参数模型的这种基本假定与实际的物理模型之间常常存在较大的差距，这些方法并非总能取得令人满意的结果。</p>
<h3 id="非参数估计"><a href="#非参数估计" class="headerlink" title="非参数估计"></a>非参数估计</h3><p>由于上述缺陷，Rosenblatt和Parzen提出了非参数估计方法，即核密度估计方法。核密度估计Kernel Density Estimation(KDE)是在概率论中用来估计未知的密度函数，属于非参数检验方法之一。</p>
<p>由于核密度估计方法不利用有关数据分布的先验知识，对数据分布不附加任何假定，是一种从数据样本本身出发研究数据分布特征的方法，因而，在统计学理论和应用领域均受到高度的重视。</p>
<p>核密度估计，就是采用平滑的峰值函数(“核”)来拟合观察到的数据点，从而对真实的概率分布曲线进行模拟。</p>
<h3 id="sklearn中实现的核函数"><a href="#sklearn中实现的核函数" class="headerlink" title="sklearn中实现的核函数"></a>sklearn中实现的核函数</h3><p>核密度估计有多种内核，如下图：<br><img src="../../../image/A0511/kde_kernels.png" alt="image"><br>sklearn核函数形式<br><img src="../../../image/A0511/sklearn_kernels.png" alt="image"><br>虽然采用不同的核函数都可以获得一致性的结论（整体趋势和密度分布规律性基本一致），但核密度函数也不是完美的。除了核算法的选择外，带宽（bandwidth）也会影响密度估计，过大或过小的带宽值都会影响估计结果。如下图(d,e,f)所示:<br><img src="../../../image/A0511/kde_bandwidth.png" alt="image"></p>
<h3 id="SVM的核函数如何选取"><a href="#SVM的核函数如何选取" class="headerlink" title="SVM的核函数如何选取"></a>SVM的核函数如何选取</h3><ol>
<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>
<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</li>
<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li>
</ol>
<h3 id="核密度估计的应用"><a href="#核密度估计的应用" class="headerlink" title="核密度估计的应用"></a>核密度估计的应用</h3><ul>
<li>股票、金融等风险预测<br>在单变量核密度估计的基础上，可以建立风险价值的预测模型。通过对核密度估计变异系数的加权处理，可以建立不同的风险价值的预测模型。</li>
<li>测量建筑密度</li>
<li>获取犯罪情况报告</li>
<li>发现对城镇或野生动物栖息地造成影响的道路或公共设施管线</li>
<li>热力图</li>
</ul>
<p>总而言之，核密度就是用来估计密度的，如果你有一系列空间点数据，那么核密度估计往往是比较好的可视化方法。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/02/28/abc/" data-id="cj8ijf1uz001xyoo0o9ev0x30" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/MachineLearning/">MachineLearning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-abc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/02/23/abc/" class="article-date">
  <time datetime="2017-02-23T00:47:00.000Z" itemprop="datePublished">2017-02-23</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/wiki/categories/DeepLearning/">DeepLearning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/wiki/2017/02/23/abc/">DL之02-深度学习中的Data Augmentation方法</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="DL之02-深度学习中的Data-Augmentation方法"><a href="#DL之02-深度学习中的Data-Augmentation方法" class="headerlink" title="DL之02-深度学习中的Data Augmentation方法"></a>DL之02-深度学习中的Data Augmentation方法</h2><hr>
<p>在深度学习中，为了避免出现过拟合（Overfitting），通常我们需要输入充足的数据量。当数据量不够大时候，常常采用以下几种方法：</p>
<ol>
<li><p>Data Augmentation：通过平移、 翻转、加噪声等方法从已有数据中创造出一批“新”的数据，人工增加训练集的大小。</p>
</li>
<li><p>Regularization：数据量比较小会导致模型过拟合, 使得训练误差很小而测试误差特别大. 通过在Loss Function 后面加上正则项可以抑制过拟合的产生。缺点是引入了一个需要手动调整的hyper-parameter。</p>
</li>
<li><p>Dropout：这也是一种正则化手段，不过跟以上不同的是它通过随机将部分神经元的输出置零来实现。详见 <a href="http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank" rel="external">http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a></p>
</li>
<li><p>Unsupervised Pre-training：用Auto-Encoder或者RBM的卷积形式一层一层地做无监督预训练, 最后加上分类层做有监督的Fine-Tuning。参考 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.207.1102&amp;rep=rep1&amp;type=pdf" target="_blank" rel="external">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.207.1102&amp;rep=rep1&amp;type=pdf</a></p>
</li>
<li><p>Transfer Learning：在某些情况下，训练集的收集可能非常困难或代价高昂。因此，有必要创造出某种高性能学习机（learner），使得它们能够基于从其他领域易于获得的数据上进行训练，并能够在对另一领域的数据进行预测时表现优异。这种方法，就是所谓的迁移学习（transfer learning）。</p>
</li>
</ol>
<h3 id="数据增强变换（Data-Augmentation-Transformation）"><a href="#数据增强变换（Data-Augmentation-Transformation）" class="headerlink" title="数据增强变换（Data Augmentation Transformation）"></a>数据增强变换（Data Augmentation Transformation）</h3><h4 id="数字图像处理中图像几何变换方法："><a href="#数字图像处理中图像几何变换方法：" class="headerlink" title="数字图像处理中图像几何变换方法："></a>数字图像处理中图像几何变换方法：</h4><ul>
<li>旋转 | 反射变换(Rotation/reflection): 随机旋转图像一定角度; 改变图像内容的朝向;</li>
<li>翻转变换(flip): 沿着水平或者垂直方向翻转图像;</li>
<li>缩放变换(zoom): 按照一定的比例放大或者缩小图像;</li>
<li>平移变换(shift): 在图像平面上对图像以一定方式进行平移;可以采用随机或人为定义的方式指定平移范围和平移步长, 沿水平或竖直方向进行平移. 改变图像内容的位置;</li>
<li>尺度变换(scale): 对图像按照指定的尺度因子, 进行放大或缩小; 或者参照SIFT特征提取思想, 利用指定的尺度因子对图像滤波构造尺度空间. 改变图像内容的大小或模糊程度;</li>
<li>对比度变换(contrast): 在图像的HSV颜色空间，改变饱和度S和V亮度分量，保持色调H不变. 对每个像素的S和V分量进行指数运算(指数因子在0.25到4之间), 增加光照变化;</li>
<li>噪声扰动(noise): 对图像的每个像素RGB进行随机扰动, 常用的噪声模式是椒盐噪声和高斯噪声;</li>
<li>颜色变换(color): 在训练集像素值的RGB颜色空间进行PCA。</li>
</ul>
<p>不同的任务背景下, 我们可以通过图像的几何变换, 使用以下一种或多种组合数据增强变换来增加输入数据的量。 几何变换不改变像素值, 而是改变像素所在的位置。 通过Data Augmentation方法扩张了数据集的范围, 作为输入时, 以期待网络学习到更多的图像不变性特征。</p>
<h4 id="Keras中的图像几何变换方法"><a href="#Keras中的图像几何变换方法" class="headerlink" title="Keras中的图像几何变换方法"></a><a href="https://keras.io/preprocessing/image/" target="_blank" rel="external">Keras中的图像几何变换方法</a></h4><p>Keras中ImageDataGenerator　实现了大多数上文中提到的图像几何变换方法。如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">keras.preprocessing.image.ImageDataGenerator(featurewise_center=<span class="keyword">False</span>,</div><div class="line">    samplewise_center=<span class="keyword">False</span>,</div><div class="line">    featurewise_std_normalization=<span class="keyword">False</span>,</div><div class="line">    samplewise_std_normalization=<span class="keyword">False</span>,</div><div class="line">    zca_whitening=<span class="keyword">False</span>,</div><div class="line">    rotation_range=<span class="number">0.</span>,</div><div class="line">    width_shift_range=<span class="number">0.</span>,</div><div class="line">    height_shift_range=<span class="number">0.</span>,</div><div class="line">    shear_range=<span class="number">0.</span>,</div><div class="line">    zoom_range=<span class="number">0.</span>,</div><div class="line">    channel_shift_range=<span class="number">0.</span>,</div><div class="line">    fill_mode=<span class="string">'nearest'</span>,</div><div class="line">    cval=<span class="number">0.</span>,</div><div class="line">    horizontal_flip=<span class="keyword">False</span>,</div><div class="line">    vertical_flip=<span class="keyword">False</span>,</div><div class="line">    rescale=<span class="keyword">None</span>,</div><div class="line">    dim_ordering=K.image_dim_ordering())</div></pre></td></tr></table></figure>
<h5 id="参数说明："><a href="#参数说明：" class="headerlink" title="参数说明："></a>参数说明：</h5><ul>
<li>featurewise_center: Boolean. Set input mean to 0 over the dataset, feature-wise.</li>
<li>featurewise_std_normalization: Boolean. Divide inputs by std of the dataset, feature-wise.</li>
<li>zca_whitening: Boolean. Apply ZCA whitening.</li>
<li>samplewise_std_normalization: Boolean. Divide each input by its std.</li>
<li>width_shift_range: Float (fraction of total width). Range for random horizontal shifts.</li>
<li>rotation_range: Int. Degree range for random rotations.</li>
<li>height_shift_range: Float (fraction of total height). Range for random vertical shifts.</li>
<li>shear_range: Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)</li>
<li>zoom_range: Float or [lower, upper]. Range for random zoom. If a float,  [lower, upper] = [1-zoom_range, 1+zoom_range].</li>
<li>channel_shift_range: Float. Range for random channel shifts.</li>
<li>fill_mode: One of {“constant”, “nearest”, “reflect” or “wrap”}. Points outside the boundaries of the input are filled according to the given mode.</li>
<li>cval: Float or Int. Value used for points outside the boundaries when fill_mode = “constant”.</li>
<li>horizontal_flip: Boolean. Randomly flip inputs horizontally.</li>
<li>vertical_flip: Boolean. Randomly flip inputs vertically.</li>
<li>rescale: rescaling factor. Defaults to None. If None or 0, no rescaling is applied, otherwise we multiply the data by the value provided (before applying any other transformation).</li>
<li>dim_ordering: One of {“th”, “tf”}. “tf” mode means that the images should have shape  (samples, height, width, channels), “th” mode means that the images should have shape  (samples, channels, height, width). It defaults to the image_dim_ordering value found in your Keras config file at  ~/.keras/keras.json. If you never set it, then it will be “tf”.</li>
</ul>
<h5 id="其它方法"><a href="#其它方法" class="headerlink" title="其它方法"></a>其它方法</h5><ul>
<li>Label shuffle: 类别不平衡数据的增广，参见海康威视ILSVRC2016的report</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/02/23/abc/" data-id="cj8ijf1up001pyoo06grga9s5" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/MI/">MI</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-abc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/02/21/abc/" class="article-date">
  <time datetime="2017-02-21T11:58:00.000Z" itemprop="datePublished">2017-02-21</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/wiki/categories/DeepLearning/">DeepLearning</a>►<a class="article-category-link" href="/wiki/categories/DeepLearning/Keras/">Keras</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/wiki/2017/02/21/abc/">Keras之04-用Cifar10数据集训练一个CNN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Keras之04-用Cifar10数据集训练一个CNN"><a href="#Keras之04-用Cifar10数据集训练一个CNN" class="headerlink" title="Keras之04-用Cifar10数据集训练一个CNN"></a>Keras之04-用Cifar10数据集训练一个CNN</h2><hr>
<h3 id="Cifar数据集介绍"><a href="#Cifar数据集介绍" class="headerlink" title="Cifar数据集介绍"></a>Cifar数据集介绍</h3><p>Cifar是加拿大政府牵头投资的一个先进科学项目研究所。Cifar-10是由Hinton的两个大弟子Alex Krizhevsky、Ilya Sutskever收集的一个用于普适物体识别的数据集。</p>
<p>Cifar-10由60000张32*32的RGB彩色图片构成，共10个分类。50000张训练，10000张测试（交叉验证）。这个数据集最大的特点在于将识别迁移到了普适物体，而且应用于多分类（姊妹数据集Cifar-100达到100类，ILSVRC比赛则是1000类）。<br><img src="../../../image/A0501/Cifar-dataset.png" alt="image"></p>
<h4 id="普适物体识别的挑战"><a href="#普适物体识别的挑战" class="headerlink" title="普适物体识别的挑战"></a>普适物体识别的挑战</h4><ul>
<li>数据中含有大量特征、噪声</li>
<li>识别物体比例不一</li>
<li>分类庞大（SVM难以应对）</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/02/21/abc/" data-id="cj8ijf1uf001lyoo0arnnb19k" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Keras/">Keras</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/MI/">MI</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-abc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/01/19/abc/" class="article-date">
  <time datetime="2017-01-19T10:23:00.000Z" itemprop="datePublished">2017-01-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/wiki/categories/DeepLearning/">DeepLearning</a>►<a class="article-category-link" href="/wiki/categories/DeepLearning/Keras/">Keras</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/wiki/2017/01/19/abc/">Keras之03-用MNIST数据集训练一个CNN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Keras之03-用MNIST数据集训练一个CNN"><a href="#Keras之03-用MNIST数据集训练一个CNN" class="headerlink" title="Keras之03-用MNIST数据集训练一个CNN"></a>Keras之03-用MNIST数据集训练一个CNN</h2><hr>
<h4 id="模型code"><a href="#模型code" class="headerlink" title="模型code"></a>模型code</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="string">'''Trains a simple convnet on the MNIST dataset.</span></div><div class="line"></div><div class="line">Gets to 99.25% test accuracy after 12 epochs</div><div class="line">(there is still a lot of margin for parameter tuning).</div><div class="line">16 seconds per epoch on a GRID K520 GPU.</div><div class="line">'''</div><div class="line"></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout, Activation, Flatten</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Convolution2D, MaxPooling2D</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div><div class="line"><span class="comment"># Keras的底层库使用Theano或TensorFlow</span></div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"></div><div class="line">batch_size = <span class="number">128</span></div><div class="line">nb_classes = <span class="number">10</span></div><div class="line">nb_epoch = <span class="number">12</span></div><div class="line"></div><div class="line"><span class="comment"># input image dimensions</span></div><div class="line">img_rows, img_cols = <span class="number">28</span>, <span class="number">28</span></div><div class="line"><span class="comment"># number of convolutional filters to use</span></div><div class="line">nb_filters = <span class="number">32</span></div><div class="line"><span class="comment"># size of pooling area for max pooling</span></div><div class="line">pool_size = (<span class="number">2</span>, <span class="number">2</span>)</div><div class="line"><span class="comment"># convolution kernel size</span></div><div class="line">kernel_size = (<span class="number">3</span>, <span class="number">3</span>)</div><div class="line"></div><div class="line"><span class="comment"># the data, shuffled and split between train and test sets</span></div><div class="line">(X_train, y_train), (X_test, y_test) = mnist.load_data()</div><div class="line"></div><div class="line"><span class="comment"># 在如何表示一组彩色图片的问题上，Theano和TensorFlow发生了分歧.</span></div><div class="line"><span class="comment"># ’th’模式，也即Theano模式会把100张RGB三通道的16×32（高为16宽为32）彩色图表示为下面这种形式（100,3,16,32），Caffe采取的也是这种方式。第0个维度是样本维，代表样本的数目，第1个维度是通道维，代表颜色通道数。后面两个就是高和宽了。</span></div><div class="line"><span class="comment"># 而TensorFlow，即’tf’模式的表达形式是（100,16,32,3），即把通道维放在了最后。</span></div><div class="line"></div><div class="line"><span class="comment"># 根据backend模式reshape输入数据</span></div><div class="line"><span class="keyword">if</span> K.image_dim_ordering() == <span class="string">'th'</span>:</div><div class="line">    X_train = X_train.reshape(X_train.shape[<span class="number">0</span>], <span class="number">1</span>, img_rows, img_cols)</div><div class="line">    X_test = X_test.reshape(X_test.shape[<span class="number">0</span>], <span class="number">1</span>, img_rows, img_cols)</div><div class="line">    input_shape = (<span class="number">1</span>, img_rows, img_cols)</div><div class="line"><span class="keyword">else</span>:</div><div class="line">    X_train = X_train.reshape(X_train.shape[<span class="number">0</span>], img_rows, img_cols, <span class="number">1</span>)</div><div class="line">    X_test = X_test.reshape(X_test.shape[<span class="number">0</span>], img_rows, img_cols, <span class="number">1</span>)</div><div class="line">    input_shape = (img_rows, img_cols, <span class="number">1</span>)</div><div class="line"></div><div class="line">X_train = X_train.astype(<span class="string">'float32'</span>)</div><div class="line">X_test = X_test.astype(<span class="string">'float32'</span>)</div><div class="line">X_train /= <span class="number">255</span></div><div class="line">X_test /= <span class="number">255</span></div><div class="line">print(<span class="string">'X_train shape:'</span>, X_train.shape)</div><div class="line">print(X_train.shape[<span class="number">0</span>], <span class="string">'train samples'</span>)</div><div class="line">print(X_test.shape[<span class="number">0</span>], <span class="string">'test samples'</span>)</div><div class="line"></div><div class="line"><span class="comment"># convert class vectors to binary class matrices</span></div><div class="line">Y_train = np_utils.to_categorical(y_train, nb_classes)</div><div class="line">Y_test = np_utils.to_categorical(y_test, nb_classes)</div><div class="line"></div><div class="line">model = Sequential()</div><div class="line"></div><div class="line"><span class="comment"># 卷积层    </span></div><div class="line"><span class="comment"># 二维卷积层对二维输入进行滑动窗卷积</span></div><div class="line"><span class="comment"># keras.layers.convolutional.Convolution2D(nb_filter, nb_row, nb_col, init='glorot_uniform', activation='linear', weights=None, border_mode='valid', subsample=(1, 1), dim_ordering='th', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)</span></div><div class="line"></div><div class="line"><span class="comment"># nb_filter：卷积核的数目,（即输出的维度）</span></div><div class="line"><span class="comment"># nb_row：卷积核的行数</span></div><div class="line"><span class="comment"># nb_col：卷积核的列数</span></div><div class="line"><span class="comment"># border_mode：边界模式，为“valid”，“same”或“full”，full需要以theano为后端</span></div><div class="line"></div><div class="line">model.add(Convolution2D(nb_filters, kernel_size[<span class="number">0</span>], kernel_size[<span class="number">1</span>],</div><div class="line">                        border_mode=<span class="string">'valid'</span>,</div><div class="line">                        input_shape=input_shape))</div><div class="line">model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">model.add(Convolution2D(nb_filters, kernel_size[<span class="number">0</span>], kernel_size[<span class="number">1</span>]))</div><div class="line">model.add(Activation(<span class="string">'relu'</span>))</div><div class="line"></div><div class="line"><span class="comment"># keras.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='th')</span></div><div class="line"><span class="comment"># 空域信号施加最大值池化</span></div><div class="line">model.add(MaxPooling2D(pool_size=pool_size))</div><div class="line">model.add(Dropout(<span class="number">0.25</span>))</div><div class="line"></div><div class="line"><span class="comment"># Flatten层用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten不影响batch的大小。</span></div><div class="line">model.add(Flatten())</div><div class="line">model.add(Dense(<span class="number">128</span>))</div><div class="line">model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">model.add(Dropout(<span class="number">0.5</span>))</div><div class="line">model.add(Dense(nb_classes))</div><div class="line">model.add(Activation(<span class="string">'softmax'</span>))</div><div class="line"></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</div><div class="line">              optimizer=<span class="string">'adadelta'</span>,</div><div class="line">              metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line">model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,</div><div class="line">          verbose=<span class="number">1</span>, validation_data=(X_test, Y_test))</div><div class="line">score = model.evaluate(X_test, Y_test, verbose=<span class="number">0</span>)</div><div class="line">print(<span class="string">'Test score:'</span>, score[<span class="number">0</span>])</div><div class="line">print(<span class="string">'Test accuracy:'</span>, score[<span class="number">1</span>])</div></pre></td></tr></table></figure>
<h4 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">Using TensorFlow backend.</div><div class="line">X_train shape: (60000, 28, 28, 1)</div><div class="line">60000 train samples</div><div class="line">10000 <span class="built_in">test</span> samples</div><div class="line">Train on 60000 samples, validate on 10000 samples</div><div class="line">Epoch 1/12</div><div class="line">60000/60000 [==============================] - 46s - loss: 0.3732 - acc: 0.8859 - val_loss: 0.0886 - val_acc: 0.9719</div><div class="line">Epoch 2/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.1350 - acc: 0.9597 - val_loss: 0.0627 - val_acc: 0.9796</div><div class="line">Epoch 3/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.1027 - acc: 0.9697 - val_loss: 0.0562 - val_acc: 0.9822</div><div class="line">Epoch 4/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0884 - acc: 0.9741 - val_loss: 0.0438 - val_acc: 0.9858</div><div class="line">Epoch 5/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0779 - acc: 0.9772 - val_loss: 0.0415 - val_acc: 0.9867</div><div class="line">Epoch 6/12</div><div class="line">60000/60000 [==============================] - 46s - loss: 0.0709 - acc: 0.9786 - val_loss: 0.0379 - val_acc: 0.9869</div><div class="line">Epoch 7/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0650 - acc: 0.9811 - val_loss: 0.0360 - val_acc: 0.9889</div><div class="line">Epoch 8/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0609 - acc: 0.9813 - val_loss: 0.0354 - val_acc: 0.9883</div><div class="line">Epoch 9/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0557 - acc: 0.9838 - val_loss: 0.0330 - val_acc: 0.9885</div><div class="line">Epoch 10/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0541 - acc: 0.9836 - val_loss: 0.0318 - val_acc: 0.9897</div><div class="line">Epoch 11/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0497 - acc: 0.9857 - val_loss: 0.0322 - val_acc: 0.9897</div><div class="line">Epoch 12/12</div><div class="line">60000/60000 [==============================] - 45s - loss: 0.0476 - acc: 0.9856 - val_loss: 0.0327 - val_acc: 0.9893</div><div class="line">Test score: 0.0326897691154</div><div class="line">Test accuracy: 0.9893</div></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/01/19/abc/" data-id="cj8ijf1u6001fyoo07b50qzwp" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Keras/">Keras</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/MI/">MI</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-abc" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/wiki/2017/01/19/abc/" class="article-date">
  <time datetime="2017-01-19T08:30:00.000Z" itemprop="datePublished">2017-01-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/wiki/categories/DeepLearning/">DeepLearning</a>►<a class="article-category-link" href="/wiki/categories/DeepLearning/Keras/">Keras</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/wiki/2017/01/19/abc/">Keras之01-用MNIST数据集训练一个DNN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Keras之01-用MNIST数据集训练一个DNN"><a href="#Keras之01-用MNIST数据集训练一个DNN" class="headerlink" title="Keras之01-用MNIST数据集训练一个DNN"></a>Keras之01-用MNIST数据集训练一个DNN</h2><hr>
<h4 id="模型code"><a href="#模型code" class="headerlink" title="模型code"></a>模型code</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -*- coding: utf-8 -*-</span></div><div class="line"></div><div class="line"><span class="string">'''Trains a simple deep NN on the MNIST dataset.</span></div><div class="line"></div><div class="line">Gets to 98.40% test accuracy after 20 epochs</div><div class="line">(there is *a lot* of margin for parameter tuning).</div><div class="line">2 seconds per epoch on a K520 GPU.</div><div class="line">'''</div><div class="line"></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">np.random.seed(<span class="number">1337</span>)  <span class="comment"># for reproducibility</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</div><div class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense, Dropout, Activation</div><div class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> RMSprop</div><div class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> np_utils</div><div class="line"></div><div class="line"></div><div class="line">batch_size = <span class="number">128</span></div><div class="line">nb_classes = <span class="number">10</span></div><div class="line">nb_epoch = <span class="number">20</span></div><div class="line"></div><div class="line"><span class="comment"># the data, shuffled and split between train and test sets</span></div><div class="line">(X_train, y_train), (X_test, y_test) = mnist.load_data()</div><div class="line"></div><div class="line"><span class="comment"># 训练数据 60000张手写图片，28*28*1</span></div><div class="line"><span class="comment"># 测试数据 10000张手写图片，28*28*1</span></div><div class="line"></div><div class="line">X_train = X_train.reshape(<span class="number">60000</span>, <span class="number">784</span>)</div><div class="line">X_test = X_test.reshape(<span class="number">10000</span>, <span class="number">784</span>)</div><div class="line">X_train = X_train.astype(<span class="string">'float32'</span>)</div><div class="line">X_test = X_test.astype(<span class="string">'float32'</span>)</div><div class="line"><span class="comment"># 归一化到0-1</span></div><div class="line">X_train /= <span class="number">255</span></div><div class="line">X_test /= <span class="number">255</span></div><div class="line">print(X_train.shape[<span class="number">0</span>], <span class="string">'train samples'</span>)</div><div class="line">print(X_test.shape[<span class="number">0</span>], <span class="string">'test samples'</span>)</div><div class="line"></div><div class="line"><span class="comment"># convert class vectors to binary class matrices</span></div><div class="line"><span class="comment"># to_categorical(y, nb_classes=None)</span></div><div class="line"><span class="comment"># 将类别向量(从0到nb_classes的整数向量)映射为二值类别矩阵, 用于应用到以categorical_crossentropy为目标函数的模型中.</span></div><div class="line"><span class="comment"># y: 类别向量; nb_classes:总共类别数</span></div><div class="line">Y_train = np_utils.to_categorical(y_train, nb_classes)</div><div class="line">Y_test = np_utils.to_categorical(y_test, nb_classes)</div><div class="line"></div><div class="line"><span class="comment"># Dense层:即全连接层</span></div><div class="line"><span class="comment"># keras.layers.core.Dense(output_dim, init='glorot_uniform', activation='linear', weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None)</span></div><div class="line"></div><div class="line"></div><div class="line">model = Sequential()</div><div class="line">model.add(Dense(<span class="number">512</span>, input_shape=(<span class="number">784</span>,)))</div><div class="line">model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">model.add(Dropout(<span class="number">0.2</span>))</div><div class="line"><span class="comment"># 激活函数可以通过设置单独的激活层实现，也可以在构造层对象时通过传递activation参数实现。</span></div><div class="line"><span class="comment"># 以下两行等价于：model.add(Dense(512,activation='relu'))</span></div><div class="line">model.add(Dense(<span class="number">512</span>))</div><div class="line">model.add(Activation(<span class="string">'relu'</span>))</div><div class="line"></div><div class="line"><span class="comment"># Dropout  需要断开的连接的比例</span></div><div class="line">model.add(Dropout(<span class="number">0.2</span>))</div><div class="line">model.add(Dense(<span class="number">10</span>))</div><div class="line">model.add(Activation(<span class="string">'softmax'</span>))</div><div class="line"></div><div class="line"><span class="comment"># 打印出模型概况</span></div><div class="line">print(<span class="string">'model.summary:'</span>)</div><div class="line">model.summary()</div><div class="line"></div><div class="line"><span class="comment"># 在训练模型之前，通过compile来对学习过程进行配置</span></div><div class="line"><span class="comment"># 编译模型以供训练</span></div><div class="line"><span class="comment"># 包含评估模型在训练和测试时的性能的指标，典型用法是metrics=['accuracy']</span></div><div class="line"><span class="comment"># 如果要在多输出模型中为不同的输出指定不同的指标，可像该参数传递一个字典，例如metrics=&#123;'ouput_a': 'accuracy'&#125;</span></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</div><div class="line">              optimizer=RMSprop(),</div><div class="line">              metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line"><span class="comment"># 训练模型</span></div><div class="line"><span class="comment"># Keras以Numpy数组作为输入数据和标签的数据类型</span></div><div class="line"><span class="comment"># fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=[], validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None)</span></div><div class="line"><span class="comment"># nb_epoch：整数，训练的轮数，训练数据将会被遍历nb_epoch次。Keras中nb开头的变量均为"number of"的意思</span></div><div class="line"><span class="comment"># verbose：日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录</span></div><div class="line"><span class="comment"># shuffle：布尔值，表示是否在训练过程中每个epoch前随机打乱输入样本的顺序。</span></div><div class="line"></div><div class="line"><span class="comment"># fit函数返回一个History的对象，其History.history属性记录了损失函数和其他指标的数值随epoch变化的情况，如果有验证集的话，也包含了验证集的这些指标变化情况</span></div><div class="line">history = model.fit(X_train, Y_train,</div><div class="line">                    batch_size=batch_size, nb_epoch=nb_epoch,</div><div class="line">                    verbose=<span class="number">1</span>, validation_data=(X_test, Y_test))</div><div class="line"></div><div class="line"></div><div class="line"><span class="comment"># evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)</span></div><div class="line"><span class="comment"># 按batch计算在某些输入数据上模型的误差</span></div><div class="line">print(<span class="string">'-------evaluate--------'</span>)</div><div class="line">score = model.evaluate(X_test, Y_test, verbose=<span class="number">1</span>)</div><div class="line">print(<span class="string">'Test score:'</span>, score[<span class="number">0</span>])</div><div class="line">print(<span class="string">'Test accuracy:'</span>, score[<span class="number">1</span>])</div></pre></td></tr></table></figure>
<h4 id="模型运行结果"><a href="#模型运行结果" class="headerlink" title="模型运行结果"></a>模型运行结果</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div></pre></td><td class="code"><pre><div class="line">Using TensorFlow backend.</div><div class="line">60000 train samples</div><div class="line">10000 <span class="built_in">test</span> samples</div><div class="line">model.summary:</div><div class="line">____________________________________________________________________________________________________</div><div class="line">Layer (<span class="built_in">type</span>)                     Output Shape          Param <span class="comment">#     Connected to                     </span></div><div class="line">====================================================================================================</div><div class="line">dense_1 (Dense)                  (None, 512)           401920      dense_input_1[0][0]              </div><div class="line">____________________________________________________________________________________________________</div><div class="line">activation_1 (Activation)        (None, 512)           0           dense_1[0][0]                    </div><div class="line">____________________________________________________________________________________________________</div><div class="line">dropout_1 (Dropout)              (None, 512)           0           activation_1[0][0]               </div><div class="line">____________________________________________________________________________________________________</div><div class="line">dense_2 (Dense)                  (None, 512)           262656      dropout_1[0][0]                  </div><div class="line">____________________________________________________________________________________________________</div><div class="line">activation_2 (Activation)        (None, 512)           0           dense_2[0][0]                    </div><div class="line">____________________________________________________________________________________________________</div><div class="line">dropout_2 (Dropout)              (None, 512)           0           activation_2[0][0]               </div><div class="line">____________________________________________________________________________________________________</div><div class="line">dense_3 (Dense)                  (None, 10)            5130        dropout_2[0][0]                  </div><div class="line">____________________________________________________________________________________________________</div><div class="line">activation_3 (Activation)        (None, 10)            0           dense_3[0][0]                    </div><div class="line">====================================================================================================</div><div class="line">Total params: 669,706</div><div class="line">Trainable params: 669,706</div><div class="line">Non-trainable params: 0</div><div class="line">____________________________________________________________________________________________________</div><div class="line">Train on 60000 samples, validate on 10000 samples</div><div class="line">Epoch 1/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.2444 - acc: 0.9243 - val_loss: 0.1180 - val_acc: 0.9642</div><div class="line">Epoch 2/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.1009 - acc: 0.9691 - val_loss: 0.0810 - val_acc: 0.9756</div><div class="line">Epoch 3/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0746 - acc: 0.9771 - val_loss: 0.0782 - val_acc: 0.9767</div><div class="line">Epoch 4/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0590 - acc: 0.9825 - val_loss: 0.0783 - val_acc: 0.9774</div><div class="line">Epoch 5/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0513 - acc: 0.9847 - val_loss: 0.0823 - val_acc: 0.9792</div><div class="line">Epoch 6/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0453 - acc: 0.9867 - val_loss: 0.0769 - val_acc: 0.9812</div><div class="line">Epoch 7/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0380 - acc: 0.9887 - val_loss: 0.0756 - val_acc: 0.9812</div><div class="line">Epoch 8/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0341 - acc: 0.9901 - val_loss: 0.0771 - val_acc: 0.9827</div><div class="line">Epoch 9/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0321 - acc: 0.9907 - val_loss: 0.0900 - val_acc: 0.9809</div><div class="line">Epoch 10/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0325 - acc: 0.9915 - val_loss: 0.0875 - val_acc: 0.9818</div><div class="line">Epoch 11/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0285 - acc: 0.9917 - val_loss: 0.0849 - val_acc: 0.9837</div><div class="line">Epoch 12/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0261 - acc: 0.9925 - val_loss: 0.0886 - val_acc: 0.9835</div><div class="line">Epoch 13/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0240 - acc: 0.9930 - val_loss: 0.1016 - val_acc: 0.9810</div><div class="line">Epoch 14/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0244 - acc: 0.9936 - val_loss: 0.0956 - val_acc: 0.9826</div><div class="line">Epoch 15/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0194 - acc: 0.9944 - val_loss: 0.0950 - val_acc: 0.9843</div><div class="line">Epoch 16/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0219 - acc: 0.9943 - val_loss: 0.1143 - val_acc: 0.9810</div><div class="line">Epoch 17/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0197 - acc: 0.9944 - val_loss: 0.1056 - val_acc: 0.9841</div><div class="line">Epoch 18/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0212 - acc: 0.9948 - val_loss: 0.1143 - val_acc: 0.9833</div><div class="line">Epoch 19/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0202 - acc: 0.9951 - val_loss: 0.1056 - val_acc: 0.9835</div><div class="line">Epoch 20/20</div><div class="line">60000/60000 [==============================] - 8s - loss: 0.0188 - acc: 0.9954 - val_loss: 0.1045 - val_acc: 0.9847</div><div class="line">-------evaluate--------</div><div class="line"> 9952/10000 [============================&gt;.] - ETA: 0sTest score: 0.104524913335</div><div class="line">Test accuracy: 0.9847</div></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://levinehuang.github.io/wiki/2017/01/19/abc/" data-id="cj8ijf1u7001gyoo06hn0fcjw" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/Keras/">Keras</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/wiki/tags/MI/">MI</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    <span class="page-number current">1</span><a class="page-number" href="/wiki/page/2/">2</a><a class="extend next" rel="next" href="/wiki/page/2/">__('next') &raquo;</a>
  </nav>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/DeepLearning/">DeepLearning</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/DeepLearning/Keras/">Keras</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/MachineLearning/">MachineLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/Tools/">Tools</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/Tools/hexo/">hexo</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/language/">language</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/language/scala/">scala</a></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/日志/">日志</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/wiki/categories/日志/运维/">运维</a></li></ul></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/AI/">AI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/Deep-Learning/">Deep Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/Keras/">Keras</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/MI/">MI</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/MachineLearning/">MachineLearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/Scala/">Scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/Spark/">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/Test/">Test</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/hexo/">hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/package/">package</a></li><li class="tag-list-item"><a class="tag-list-link" href="/wiki/tags/pip/">pip</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/wiki/tags/AI/" style="font-size: 20px;">AI</a> <a href="/wiki/tags/Deep-Learning/" style="font-size: 20px;">Deep Learning</a> <a href="/wiki/tags/Keras/" style="font-size: 15px;">Keras</a> <a href="/wiki/tags/MI/" style="font-size: 20px;">MI</a> <a href="/wiki/tags/MachineLearning/" style="font-size: 10px;">MachineLearning</a> <a href="/wiki/tags/Python/" style="font-size: 10px;">Python</a> <a href="/wiki/tags/Scala/" style="font-size: 15px;">Scala</a> <a href="/wiki/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/wiki/tags/Test/" style="font-size: 10px;">Test</a> <a href="/wiki/tags/hexo/" style="font-size: 10px;">hexo</a> <a href="/wiki/tags/package/" style="font-size: 10px;">package</a> <a href="/wiki/tags/pip/" style="font-size: 10px;">pip</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/wiki/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/wiki/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/wiki/archives/2017/03/">March 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/wiki/archives/2017/02/">February 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/wiki/archives/2017/01/">January 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/wiki/archives/2016/12/">December 2016</a></li><li class="archive-list-item"><a class="archive-list-link" href="/wiki/archives/2016/11/">November 2016</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/wiki/2017/08/08/A05-MI/02-TensorFlow/multilayer_perceptron/">(no title)</a>
          </li>
        
          <li>
            <a href="/wiki/2017/05/07/A05-MI/03-SparkMLlib/SparkMLlib之01-Spark机器学习库介绍/">(no title)</a>
          </li>
        
          <li>
            <a href="/wiki/2017/03/22/A01-BigData/02-Spark/SparkSQL/">(no title)</a>
          </li>
        
          <li>
            <a href="/wiki/2017/03/09/A05-MI/10-DL理论/DL之05-RNN/">(no title)</a>
          </li>
        
          <li>
            <a href="/wiki/2017/03/01/A05-MI/10-DL理论/LSTM/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Levine Huang<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/wiki/" class="mobile-nav-link">Home</a>
  
    <a href="/wiki/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/wiki/fancybox/jquery.fancybox.css">
  <script src="/wiki/fancybox/jquery.fancybox.pack.js"></script>


<script src="/wiki/js/script.js"></script>

  </div>
</body>
</html>